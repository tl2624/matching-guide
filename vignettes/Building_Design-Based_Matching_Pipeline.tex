% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
  12pt,
  leqno]{article}
\usepackage{xcolor}
\usepackage[margin = 1.5cm]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage[]{natbib}
\bibliographystyle{apsr}
\usepackage{xcolor, color, ucs}     % http://ctan.org/pkg/xcolor
\usepackage{natbib}
\usepackage{booktabs}          % package for thick lines in tables
\usepackage{amsfonts,amsthm,amsmath}          % AMS Fonts
\usepackage{mathtools}
\usepackage{graphicx}          % Insert .pdf, .eps or .png
\usepackage{enumitem}          % http://ctan.org/pkg/enumitem
\usepackage{tabularx}          % Get scale boxes for tables
\usepackage{float}             % Force floats around
\usepackage{placeins}          % For use of \FloatBarrier 
\usepackage{rotating}          % Rotate long tables horizontally
\usepackage{bbm}               
\usepackage{bm}                
\usepackage{csquotes}           % \enquote{} and \textquote[][]{} environments
\usepackage{subfigure}
\usepackage{array}

\usepackage{longtable}


\setlist{nosep}

\usepackage{setspace}
\doublespacing



\usepackage{multirow}

\usepackage{hyperref}
\usepackage[noabbrev]{cleveref} % Should be loaded after \usepackage{hyperref}
\usepackage[small,bf]{caption}  % Captions
\captionsetup[table]{position=bottom} % force captions below tables


\parskip=8pt
\parindent=0pt
\delimitershortfall=-1pt
\interfootnotelinepenalty=100000

\newcommand{\qedknitr}{\hfill\rule{1.2ex}{1.2ex}}

%

\def\tightlist{}

\makeatletter
% align all math after the command
\newcommand{\mathleft}{\@fleqntrue\@mathmargin\parindent}
\newcommand{\mathcenter}{\@fleqnfalse}
% tilde with text over it
\newcommand{\distas}[1]{\mathbin{\overset{#1}{\kern\z@\sim}}}%
\newsavebox{\mybox}\newsavebox{\mysim}
\newcommand{\distras}[1]{%
  \savebox{\mybox}{\hbox{\kern3pt$\scriptstyle#1$\kern3pt}}%
  \savebox{\mysim}{\hbox{$\sim$}}%
  \mathbin{\overset{#1}{\kern\z@\resizebox{\wd\mybox}{\ht\mysim}{$\sim$}}}%
}
\makeatother

% \newtheoremstyle{newstyle}
% {} %Aboveskip
% {} %Below skip
% {\mdseries} %Body font e.g.\mdseries,\bfseries,\scshape,\itshape
% {} %Indent
% {\bfseries} %Head font e.g.\bfseries,\scshape,\itshape
% {.} %Punctuation afer theorem header
% { } %Space after theorem header
% {} %Heading

\newtheorem{thm}{Theorem}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{definition}{Definition}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand*\Diff[1]{\mathop{}\!\mathrm{d^#1}}
\DeclareMathOperator{\E}{\mathrm{E}}
\DeclareMathOperator{\Var}{\mathrm{Var}}
\DeclareMathOperator{\R}{\mathbb{R}}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

% suppress table numbering
\captionsetup[table]{labelformat=empty}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Building a Design-Based Matching Pipeline: From Principles to Practical Implementation in R},
  pdfauthor={ and },
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Building a Design-Based Matching Pipeline: From Principles to
Practical Implementation in R}
\author{\href{mailto:thomas.leavitt@baruch.cuny.edu}{Thomas Leavitt} and
\href{mailto:luke_miratrix@gse.harvard.edu }{Luke W. Miratrix}}
\date{}

\begin{document}
\maketitle

\begin{abstract}
\singlespacing
\noindent Matching, a canonical design for observational studies, takes many forms that often rest on distinct --- yet implicit --- statistical principles. We construct a matching pipeline for practitioners that makes these principles explicit by grounding each step in a coherent design-based framework. The pipeline begins with the conceptual ideal of a randomized experiment, traces how observational studies deviate from that ideal, and uses matching to approximate it. The next stage is inference under the as-if randomization assumption of matched sets' being equivalent to a collection of miniature randomized experiments within blocks. Under this assumption, we consider inference on all individual effects in the "sharp" framework and the average effect in the "weak" framework. The final stage is a sensitivity analysis to assess, under either framework, how inferences change under departures from as-if randomization. Each step includes extensively commented \texttt{R} code that equips practitioners to implement both established and newly developed procedures, including several not yet available in existing \texttt{R} packages. By integrating methods that are often considered separately into an overall pipeline, we aim to help practitioners understand why each step matters and how the pipeline can be tailored to their own data. We illustrate the full workflow through an application examining the effect of United Nations peacekeeping interventions on the duration of post-conflict peace.
\end{abstract}

\newpage

\section{Design-Based Foundations of the Matching
Pipeline}\label{design-based-foundations-of-the-matching-pipeline}

\subsection{The Randomized Experimental
Ideal}\label{the-randomized-experimental-ideal}

Imagine a randomized experiment in which a researcher flips a fair coin
independently for each individual in the study. Heads means assignment
of the individual to control, while tails means assignment to treatment.
After assignments to treatment and control, the researcher administers
the conditions and then compares outcomes between treated and control
groups.

Why is this procedure effective? Randomization is a \emph{fair lottery}:
Every individual has the same chance of being assigned to treatment.
This means that individuals who would respond more strongly to treatment
are no more likely to receive it than those who would respond less
strongly, and the same is true for control. Because each individual's
assignment is determined by the same coin toss (with the same
probability of landing heads or tails), randomization leaves only two
possibilities: (1) the difference in outcomes between treatment and
control groups reflects the true causal effect, or (2) chance variation
produced a misleading difference. Although misleading differences can
occur by chance, randomization is valuable because it enables us to use
statistical tools to quantify and limit the chance of such errors. As a
result, randomized experiments yield especially credible causal
conclusions.

Randomization is useful not only as a procedure, but also as an idea. It
helps us understand when statistical tools will (and will not) yield
credible conclusions, even when we have not directly randomized. In an
observational study, the researcher does not control who receives the
treatment and instead observes units after they have been assigned to
treatment and control groups. In such settings, the idea of
randomization can guide how we design studies so that they yield more
credible causal conclusions.

\subsection{Bridging Randomized Experiments and Observational
Studies}\label{bridging-randomized-experiments-and-observational-studies}

A useful framework for connecting randomized experiments to
observational studies is what \citet{rubin1977} calls ``assignment to
treatment group on the basis of a covariate,'' where a covariate is a
pre-treatment characteristic of a study's units. \citet{rubin1977}
supposes independent coin tosses for each individual in which the
probability of heads or tails now depends on that individual's value of
a single covariate. Consequently, all individuals with the same
covariate value share the same chance of ending up in treatment, while
those with a different covariate value share a different chance.

In this setting, we can envision forming groups (i.e., matched sets) so
that all individuals within a group share the same covariate value. Each
group then functions as a miniature randomized experiment in that random
chance alone explains why some individuals in the group ended up in
treatment while others did not. Importantly, to justify this
interpretation, we do not need to know each individual's actual
probability of treatment. It is enough to know that the probability of
treatment depends only on the covariate, which ensures that all
individuals with the same covariate value have the same chance of
treatment.

\subsection{Matching to Approximate the Randomized Experimental
Ideal}\label{matching-to-approximate-the-randomized-experimental-ideal}

The same intuition applies to matching when the probability of treatment
depends on many baseline covariates. The underlying idea is that
individuals with similar covariates have similar treatment assignment
probabilities. Thus, in an effort to recreate a randomized experiment,
we use the covariates we observe and believe determine treatment chances
in order to divide individuals into groups, with each group containing
both treated and control subjects who are homogeneous in those
covariates. We are, in effect, constructing a new single variable: group
(i.e., matched set) membership. We can think of this membership variable
as the single covariate in the framework of ``assignment to treatment
group on the basis of a covariate'' \citep{rubin1977} discussed above.
Although we still do not know each individual's treatment probability,
the hope is that all individuals within a group share the same
probability, whatever that probability may be.

We refer to the condition that all individuals within matched sets share
the same treatment probability as \emph{as-if randomization}, although
other terms, such as \emph{selection on observables}, are also common.
We use the term as-if randomization because, when all units in a matched
set share the same treatment probability, conditioning on the number of
treated units makes every possible assignment within the set equally
likely, creating a situation that is as if we had actually randomized.
When as-if randomization holds, we can use the same statistical tools
that we would use in a randomized experiment to draw credible causal
inferences from our observational study.

\subsection{Why Sensitivity Analysis
Matters}\label{why-sensitivity-analysis-matters}

Unlike a randomized experiment, even the best matched designs rely on
the strong assumption of as-if randomization. When this assumption
holds, we can draw causal conclusions by analyzing the data as if they
came from a randomized experiment. However, if the assumption is wrong,
our causal claims are no longer guaranteed to be credible.

How can this assumption fail? First, individuals within a group may be
similar on observed covariates, but not similar enough to have the same
treatment probabilities. Second, treatment assignment may depend on
covariates we did not measure. If so, even if individuals within groups
appear comparable on observed covariates, those individuals may still
differ on hidden covariates that determine the probability of treatment.

For these reasons, it is important to assess the sensitivity of our
causal conclusions to departures from as-if randomization. Conclusions
are especially convincing when they hold not only under this assumption,
but also under moderate violations of it. Conclusions that collapse
under only mild departures are much less convincing.

\subsection{Roadmap of the Design-Based Matching
Pipeline}\label{roadmap-of-the-design-based-matching-pipeline}

Building on these design-based foundations, we now outline a pipeline
that starts with matching and then proceeds to inference and sensitivity
analysis:

\begin{itemize}
\item
  \textbf{Construct and evaluate matched sets}. We begin with the
  mechanics of optimal matching \citep{hansen2004, hansenklopfer2006}:
  choosing a distance measure that defines similarity on the covariates,
  setting calipers --- maximum allowable distances between treated and
  control units for inclusion in the same matched set --- and imposing
  structural constraints (e.g., requiring matches to be pairs). We then
  show how to evaluate the resulting design in terms of both effective
  sample size and covariate balance. In particular, we focus on tests
  proposed by \citet{hansenbowers2008} that compare the matched design's
  covariate balance to what one would expect under an equivalent
  completely randomized experiment within blocks (that is, random
  assignment with a fixed number of treated units per block).
\item
  \textbf{Draw causal inferences}. Once a matched design is chosen,
  practitioners can conduct inference under either a ``sharp'' causal
  framework, which pertains to individual-level effects for all units,
  or a ``weak'' framework, which pertains to a summary quantity of the
  unit-level causal effects, typically the average treatment effect
  (ATE). For the sharp framework, we focus on how researchers can use
  both simulation- and Normal-based approximations to the exact
  randomization distribution, either to perform hypothesis tests or to
  obtain point estimates by inverting those tests. For the weak
  framework, we cover estimation of the ATE and hypothesis tests about
  it. We emphasize exposition and code for recent variance estimators
  tailored to designs with only \(1\) treated or \(1\) control unit per
  matched set \citep{fogarty2018, pashleymiratrix2021} --- an important
  case, since such designs are optimal in terms of balance and effective
  sample size \citep{gurosenbaum1993, rosenbaum1991, hansen2004}.
\item
  \textbf{Assess sensitivity}. Finally, we turn to sensitivity analyses
  under both inferential frameworks. We review established methods for
  conducting sensitivity analysis for tests of sharp nulls under
  possible violations of as-if randomization
  \citep{rosenbaumkrieger1990, gastwirthetal2000, rosenbaum2018}. We
  then describe and implement new methods that extend sensitivity
  analysis to tests of weak null hypotheses \citep{fogarty2023}.
\end{itemize}

Below we present a flow diagram that summarizes the overall pipeline.
The diagram shows each step and decision point, the relevant \texttt{R}
tools (whether existing packages or custom functions included herein)
and core references.

\singlespacing

\begin{center}\includegraphics[width=\linewidth]{fig/matching_pipeline_flowchart} \end{center}
\doublespacing

\subsection{Related Topics Beyond Our
Scope}\label{related-topics-beyond-our-scope}

For clarity and focus, we exclude several important topics, though we
point to references where relevant. Some of the topics we exclude are
not specific to matching or observational studies. These topics include
imperfect compliance with treatment assignment, missing outcomes,
clustered (rather than individual) assignment, and interference in which
a subject's outcome depends on others' treatment assignments.

We focus on inferences about only two types of causal effects: a
constant, additive effect for all units and the average effect. We do
not consider other effect models, such as dilated \citep{rosenbaum1999},
multiplicative or tobit effects \citep[pp.~46-49]{rosenbaum2010}. In
what follows, we treat causal targets as fixed (over possible
assignments consistent with the matched design) rather than as random
quantities. Thus, we do not cover attributable effects
\citep{rosenbaum2001, rosenbaum2003, hansenbowers2009} or the average
treatment effect on the treated \citep{sekhonshem-tov2021}. We also set
aside methods that jointly infer both types of causal effects
\citep{ding2017, wuding2021, cohenfogarty2022}, as well as strategies
for improving the power of hypothesis tests by rescaling outcomes or
choosing different test statistics (functions that summarize data to
test hypotheses), e.g., with regression-based tools
\citep{lin2013, cohenfogarty2023, guobasse2023}.

For issues specific to matching, the most noteworthy omissions are as
follows:

\begin{itemize}
\item
  \textbf{Additional matching and balancing approaches}. We do not cover
  nonbipartite matching for multi-valued treatments
  \citep[pp.~207--221]{luetal2001, luetal2011, rosenbaum2010}, template
  \citep{silberetal2014}, multilevel
  \citep{zubizarretakeele2017, pimenteletal2018}, risk-set
  \citep{lietal2001}, cardinality \citep{zubizarretaetal2014}, coarsened
  exact \citep{iacusetal2012}, or generalized full matching
  \citep{savjeetal2021}. We also omit balance constraints such as fine
  and near-fine balance \citep{rosenbaumetal2007, yangetal2012}, which
  ensure exact (or nearly exact) equality in marginal distributions of
  categorical covariates across groups. The principles we present,
  however, apply broadly, and our pipeline can be readily adapted to
  these additional methods.
\item
  \textbf{Prognostic covariates}. We do not cover methods that use pilot
  or external data to figure out which covariates best predict outcomes
  and then incorporate them into matching. For theoretical foundations,
  see \citet{hansen2008a}. For a clear exposition of both how to
  implement such methods and why they improve inference, see
  \citet{salesetal2018}.
\item
  \textbf{Design sensitivity}. We do not include methods that form
  matched sets to improve design sensitivity --- that is, to make
  inferences more robust to moderate hidden bias. Beyond the
  construction of matches, researchers can also enhance design
  sensitivity by selecting particular test statistics, among other
  strategies
  \citep{rosenbaum2004, helleretal2009, hsuetal2013, smalletal2013}.
\item
  \textbf{Extensions to balance testing}. We omit extensions of the
  covariate balance testing framework in \citet{hansenbowers2008},
  including those developed in \citet{gagnon-bartschshem-tov2019} and
  \citet{branson2021}, and related approaches, such as those building on
  the stepwise-intersection-union principle (SIUP)
  \citep{hansensales2015}.
\item
  \textbf{Residual imbalance on observed covariates}. We make the
  simplifying assumption that membership in a matched set is sufficient
  to achieve as-if randomization, at least with respect to observed
  covariates. We therefore subsume any residual imbalances on observed
  covariates after matching into the sensitivity analysis for imbalances
  on hidden covariates. However, residual imbalances on observed
  covariates are not truly ``hidden,'' and several approaches propose
  alternatives that explicitly account for such imbalances, either in
  methods targeting individual effects
  \citep{rosenbaum1988, pimentelhuang2024, chenetal2023, hengetal2025}
  or in those targeting average effects \citep{zhuetal2025}.
\item
  \textbf{\(\bm{Z}\)-dependence}. In some settings, matched set
  membership may itself change over treatment assignments
  \citep{pashleyetal2021, pimentelhuang2024, pimentelyu2024}. Our
  exposition relies on an analogy to Rubin's framework of ``assignment
  to treatment group on the basis of a covariate'' \citep{rubin1977},
  where matched set membership plays the role of the covariate. This
  analogy assumes that membership is fixed, but in practice it could be
  a random variable if the matched structure depends on the observable
  assignments. We set aside this issue, which may be minor when
  practitioners use sufficiently tight calipers for matching.
\end{itemize}

\section{Implementation of the Matching
Pipeline}\label{implementation-of-the-matching-pipeline}

We provide a high-level overview of the ideas behind matching and
include code that demonstrates how to implement those ideas with various
\texttt{R} packages. We also work through some of these steps ``by
hand'' to underscore the underlying conceptual issues. Doing so also
gives practitioners more flexibility to adapt the matching pipeline to
their own needs.

We break the implementation into specific decision points that
practitioners commonly face, and present the pipeline in three main
parts (see Figure 1):

\begin{enumerate}
\item Part 1: Making a Matched Dataset with Comparable Treatment and Control Groups
\begin{enumerate}
\item How Do I Measure Similarity on My Chosen Covariates?
\begin{itemize}
\item Similarity on the Estimated Propensity Score
\end{itemize}
\item How Can I Apply Rules for Matches to Ensure Comparability?
\item How Can I Apply Rules for Matches to Improve Effective Sample Size?
\item How Do I Actually Form the Matches?
\item How Do I Decide Whether to Move Ahead with My Matched Design?
\end{enumerate}
\item Part 2: Causal Inference from the Matched Design (under As-If Randomization)
\begin{enumerate}
\item How Do I Draw Inferences under Sharp Framework?
\item How Do I Draw Inferences under Weak Framework?
\end{enumerate}
\item Part 3: Sensitivity Analysis for Hidden Confounding (Departures from As-If Randomization)
\begin{enumerate}
\item How Do My Inferences under the Sharp Framework Change under these Departures?
\begin{itemize}
\item Finding the Worst-Case Scenario of Hidden Confounding to Ensure Valid Inference
\begin{itemize}
\item Separable Approximation
\item Taylor Series Approximation
\end{itemize}
\item Conducting Sensitivity Analysis under the Worst-Case Scenario
\end{itemize}
\item How Do My Inferences under the Weak Framework Change under these Departures?  
\end{enumerate}
\end{enumerate}

Before turning to Part 1, we introduce a running example taken from
\citet{gilligansergenti2008}, which we use throughout this document.

\subsection{Running Example: United Nations Peacekeeping and
Post-Conflict
Peace}\label{running-example-united-nations-peacekeeping-and-post-conflict-peace}

We introduce matching through an example that examines the causal effect
of United Nations (UN) peacekeeping missions on the durability of
post-conflict peace, a question of central importance for both academic
research and policy. Our example draws on data from
\citet{gilligansergenti2008}, whose title includes the phrase ``Matching
to Improve Causal Inference,'' underscoring the value of applying
matching to study the UN's causal impact on post-conflict peace. These
data are publicly available in the supplementary information of the
article's webpage in the \emph{Quarterly Journal of Political Science}
(DOI:
\href{https://doi.org/10.1561/100.00007051}{10.1561/100.00007051}).

The dataset from \citet{gilligansergenti2008} includes 87 observations,
each corresponding to a country's peace-period episode following a civil
war, with episodes beginning as early as January 1989 and data extending
through December 2003. In some episodes, UN peacekeepers intervened
(e.g., Sierra Leone, Jan 2001 - Dec 2003), while in others they did not
(e.g., Macedonia, Sep 2001 - Dec 2003). The treatment variable is UN
intervention (\texttt{UN}), coded as \(1\) if a UN mission was present
during the peace period and \(0\) otherwise. The outcome variable is the
duration of the peace spell, which \citet{gilligansergenti2008} measure
as the log of the number of days from the start of peace until either
the outbreak of a new conflict or right-censoring at December 2003. This
log-transformed outcome (\texttt{ldur}) captures the total length of the
peace period rather than the time elapsed after a potential UN
intervention. In practice, however, UN interventions almost always began
immediately after the onset of peace: ``Of the 19 post-conflict UN
interventions, the United Nations was present within the first month for
16 of them'' \citep[p.~118]{gilligansergenti2008}. Going forward, we set
aside these two measurement details.

As \citet{gilligansergenti2008} state, ``UN missions are not randomly
assigned'' (p.~89). Whether peacekeepers are present in a country during
a given peace period depends on baseline covariates such as the logged
number of deaths (\texttt{lwdeaths}), the logged duration
(\texttt{lwdurat}) of the last war, ethnic fractionalization
(\texttt{ethfrac}, a 0 - 1 index intended to represent the chance that
two randomly chosen individuals belong to different ethnic groups),
logged population size (\texttt{pop}) and others. We implement matching
using these same covariates, but emphasize that our exercise is
expository and not intended as a replication of the original findings.

\subsubsection{Loading the Data for the Running
Example}\label{loading-the-data-for-the-running-example}

To load the data, you could first download the replication files from
the supplementary information on the article's webpage (DOI:
\href{https://doi.org/10.1561/100.00007051}{10.1561/100.00007051}), save
the files to your working directory, and then use a package such as
\texttt{haven} to load the Stata (.dta) file,
\texttt{peace\_pre\_match.dta}, into \texttt{R}. However, for our
purposes we recommend loading our pre-created .RData file
(\texttt{peace\_pre\_match.RData}), in which Stata's monthly numeric
dates have been converted to \texttt{R}'s year-month format and the
geographic region indicators recoded into a single factor variable
(\texttt{region}). The command below loads this pre-created dataset
(\texttt{peace\_pre\_match.RData}) into the \texttt{R} environment as an
object named \texttt{data}.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the readr package (for reading and writing .rds files)}
\CommentTok{\# install.packages(\textquotesingle{}readr\textquotesingle{}) \# Run this line if it is not already installed}
\FunctionTok{library}\NormalTok{(readr)}

\CommentTok{\# Load the cleaned dataset}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read\_rds}\NormalTok{(}\AttributeTok{file =}\NormalTok{ rds\_path)}
\end{Highlighting}
\end{Shaded}

\doublespacing

\subsection{Part 1: Making a Matched Dataset with Comparable Treatment
and Control
Groups}\label{part-1-making-a-matched-dataset-with-comparable-treatment-and-control-groups}

In an observational setting, treatment is not assigned by the flip of a
coin but depends on individuals' covariates. The goal of matching is to
compare treated and control units that have the same chances of
treatment based on those covariates --- i.e., the same \emph{propensity
scores}. If we could observe propensity scores, it would be
straightforward to compare treated and control units by matching
directly on them. Because propensity scores are not directly observed,
we instead aim to create a collection of matched sets that is
\emph{balanced} --- meaning that treated and control observations are
similar in their covariates.

\subsubsection{How Do I Measure Similarity on My Chosen
Covariates?}\label{how-do-i-measure-similarity-on-my-chosen-covariates}

In the simplest terms, matching is about ensuring apples-to-apples,
rather than apples-to-oranges, comparisons between treated and control
observations \citep{rubinwaterman2006}. To create matched sets in which
treated and control groups are similar in their covariates, we first
need a distance measure that quantifies how close any two observations
are. With such a measure in hand, we can then construct sets of treated
and control observations that are close on this measure --- i.e.,
apples-to-apples in their pre-treatment characteristics.

We record the distances between treated and control units in a distance
matrix: The rows correspond to treated units and the columns to control
units. Each cell of the matrix records the distance between a specific
treated unit and a specific control unit, as defined by a distance
measure. This distance measure takes the baseline covariates of the two
units and maps them to a single nonnegative number, with smaller values
indicating greater similarity.

In our setting, we are interested in similarity across \(9\) covariates,
named in the object \texttt{covs}.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define character vector of the 9 covariate names in the dataset}
\NormalTok{covs }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"lwdeaths"}\NormalTok{, }\StringTok{"lwdurat"}\NormalTok{, }\StringTok{"ethfrac"}\NormalTok{, }\StringTok{"pop"}\NormalTok{, }\StringTok{"lmtnest"}\NormalTok{, }\StringTok{"milper"}\NormalTok{, }\StringTok{"bwgdp"}\NormalTok{,}
          \StringTok{"bwplty2"}\NormalTok{, }\StringTok{"region"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\doublespacing

The first four --- \texttt{lwdeaths}, \texttt{lwdurat},
\texttt{ethfrac}, and \texttt{pop} --- were introduced earlier. The
others include a logged measure of the proportion of a country's land
area that is mountainous (\texttt{lmtnest}), the logged total number of
military personnel in a country (\texttt{milper}), logged GDP per capita
before the last civil war (\texttt{bwgdp}), the Polity score (a standard
--10 to 10 scale of democratic versus autocratic institutions) before
the last civil war (\texttt{bwplty2}), and a region factor
(\texttt{region}) with categories for Eastern Europe, Latin America,
Asia, Sub-Saharan Africa, and North Africa/Middle East.

All of these covariates are measured before treatment and presumably
determine the chance of a UN intervention during a country's peace
period. Our interest in them stems primarily from their role in
determining those intervention probabilities. Yet many of these
covariates may also be prognostic; that is, they help predict the
outcome of interest in \citet{gilligansergenti2008}, the log duration of
the peace period (\texttt{ldur}) that countries would potentially
experience with or without a UN intervention. This prognostic value of
covariates can provide an additional reason to match on them
\citep{hansen2008a, salesetal2018}.

There are many ways to measure the distance between a treated and a
control unit. For example, we might compare units using the Euclidean
distance --- i.e., the square root of the sum of squared differences ---
across all baseline covariates. To do so, we first convert the factor
variable \texttt{region}, which stores categorical labels, into a set of
dummy (\(0\) or \(1\)) variables, one for each region. This conversion
ensures that distances between treated and control units can be
computed, since distance measures require numeric variables rather than
categorical labels.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Install "dplyr" package (only run if you don\textquotesingle{}t already have it installed)}
\CommentTok{\# install.packages("dplyr")}

\CommentTok{\# Load dplyr package for data manipulation (mutate, group\_by, summarize, etc.)}
\FunctionTok{library}\NormalTok{(dplyr)}

\CommentTok{\# Convert categorical variable \textquotesingle{}region\textquotesingle{} into 0/1 dummy indicators}
\NormalTok{data }\OtherTok{\textless{}{-}}\NormalTok{ data }\SpecialCharTok{|\textgreater{}} \CommentTok{\# Pipe (|\textgreater{}) to pass left{-}hand result into next function call}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{eeurop   =} \FunctionTok{ifelse}\NormalTok{(}\AttributeTok{test =}\NormalTok{ region }\SpecialCharTok{==} \StringTok{"eeurop"}\NormalTok{,   }\AttributeTok{yes =} \DecValTok{1}\NormalTok{, }\AttributeTok{no =} \DecValTok{0}\NormalTok{),}
    \AttributeTok{lamerica =} \FunctionTok{ifelse}\NormalTok{(}\AttributeTok{test =}\NormalTok{ region }\SpecialCharTok{==} \StringTok{"lamerica"}\NormalTok{, }\AttributeTok{yes =} \DecValTok{1}\NormalTok{, }\AttributeTok{no =} \DecValTok{0}\NormalTok{),}
    \AttributeTok{asia     =} \FunctionTok{ifelse}\NormalTok{(}\AttributeTok{test =}\NormalTok{ region }\SpecialCharTok{==} \StringTok{"asia"}\NormalTok{,     }\AttributeTok{yes =} \DecValTok{1}\NormalTok{, }\AttributeTok{no =} \DecValTok{0}\NormalTok{),}
    \AttributeTok{ssafrica =} \FunctionTok{ifelse}\NormalTok{(}\AttributeTok{test =}\NormalTok{ region }\SpecialCharTok{==} \StringTok{"ssafrica"}\NormalTok{, }\AttributeTok{yes =} \DecValTok{1}\NormalTok{, }\AttributeTok{no =} \DecValTok{0}\NormalTok{),}
    \AttributeTok{nafrme   =} \FunctionTok{ifelse}\NormalTok{(}\AttributeTok{test =}\NormalTok{ region }\SpecialCharTok{==} \StringTok{"nafrme"}\NormalTok{,   }\AttributeTok{yes =} \DecValTok{1}\NormalTok{, }\AttributeTok{no =} \DecValTok{0}\NormalTok{)}
\NormalTok{  )}

\CommentTok{\# Remove \textquotesingle{}region\textquotesingle{} and replace with dummy indicators}
\NormalTok{expanded\_covs }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}
  \FunctionTok{setdiff}\NormalTok{(}\AttributeTok{x =}\NormalTok{ covs, }\AttributeTok{y =} \StringTok{"region"}\NormalTok{),}
  \StringTok{"eeurop"}\NormalTok{, }\StringTok{"lamerica"}\NormalTok{, }\StringTok{"asia"}\NormalTok{, }\StringTok{"ssafrica"}\NormalTok{, }\StringTok{"nafrme"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\doublespacing

Here we calculate the Euclidean distance between post-conflict Liberia,
where the UN did intervene, and post-conflict Guinea-Bissau, where the
UN did not.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract covariate values for Liberia and Guinea{-}Bissau (cname = country name)}
\NormalTok{liberia }\OtherTok{\textless{}{-}}\NormalTok{ data[data}\SpecialCharTok{$}\NormalTok{cname }\SpecialCharTok{==} \StringTok{"Liberia"}\NormalTok{, expanded\_covs]}
\NormalTok{guinea\_bissau }\OtherTok{\textless{}{-}}\NormalTok{ data[data}\SpecialCharTok{$}\NormalTok{cname }\SpecialCharTok{==} \StringTok{"Guinea{-}Bissau"}\NormalTok{, expanded\_covs]}

\CommentTok{\# Compute Euclidean distance between the two countries on these covariates}
\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{((liberia }\SpecialCharTok{{-}}\NormalTok{ guinea\_bissau)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)) }\CommentTok{\# Display the Euclidean distance}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 57.38306
\end{verbatim}

\doublespacing

We can obtain the full matrix of pairwise distance values using
\texttt{match\_on()} from the \texttt{optmatch} package. We do not need
to manually recode factor variables into dummy indicators because
\texttt{match\_on()} handles this conversion automatically.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a formula: UN (treatment indicator) \textasciitilde{} covariates}
\CommentTok{\# Note: we keep "region" in covs as a factor}
\NormalTok{cov\_fmla }\OtherTok{\textless{}{-}} \FunctionTok{reformulate}\NormalTok{(}\AttributeTok{termlabels =}\NormalTok{ covs,}
                        \AttributeTok{response =} \StringTok{"UN"}\NormalTok{)}

\CommentTok{\# Install optmatch if not already installed}
\CommentTok{\# install.packages("optmatch")}

\CommentTok{\# Load optmatch, which provides the match\_on() function}
\FunctionTok{library}\NormalTok{(optmatch)}

\CommentTok{\# Compute Euclidean distance matrix between treated (UN = 1) and control (UN = 0)}
\NormalTok{dist\_mat\_euc }\OtherTok{\textless{}{-}} \FunctionTok{match\_on}\NormalTok{(}\AttributeTok{x =}\NormalTok{ cov\_fmla,                 }\CommentTok{\# formula for covariates}
                         \AttributeTok{data =}\NormalTok{ data,                  }\CommentTok{\# dataset used}
                         \AttributeTok{standardization.scale =} \ConstantTok{NULL}\NormalTok{, }\CommentTok{\# no rescaling of covariates}
                         \AttributeTok{method =} \StringTok{"euclidean"}\NormalTok{)         }\CommentTok{\# use Euclidead distance}

\CommentTok{\# Add country names (cname) as row/column labels for clarity}
\FunctionTok{dimnames}\NormalTok{(dist\_mat\_euc) }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{cname[data}\SpecialCharTok{$}\NormalTok{UN }\SpecialCharTok{==} \DecValTok{1}\NormalTok{], data}\SpecialCharTok{$}\NormalTok{cname[data}\SpecialCharTok{$}\NormalTok{UN }\SpecialCharTok{==} \DecValTok{0}\NormalTok{])}

\CommentTok{\# Display a submatrix of distances: treated units 11–15 vs control units 27–30}
\FunctionTok{round}\NormalTok{(}\AttributeTok{x =}\NormalTok{ dist\_mat\_euc[}\DecValTok{11}\SpecialCharTok{:}\DecValTok{15}\NormalTok{, }\DecValTok{27}\SpecialCharTok{:}\DecValTok{30}\NormalTok{], }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
              Niger Guinea   Togo Central African Republic
Sierra Leone  94.14 101.16 116.18                   116.65
Zaire         20.64  25.71  41.44                    41.29
Rwanda        28.56  35.69  50.54                    51.74
Mozambique   133.24 140.28 155.30                   155.77
Namibia      246.38 253.13 268.33                   268.08
\end{verbatim}

\doublespacing

In this little subset of the full distance matrix, the first entry is
the Euclidean distance of Sierra Leone (treated) from Niger (control).
The last listed entry is the distance between Namibia (treated) and the
Central African Republic (control).

One concern with Euclidean distance is that it depends on the scale of
the covariates. For example, the difference between a country in
Sub-Saharan Africa (\texttt{ssafrica} = 1) and a country in Latin
America (\texttt{ssafrica} = 0) would contribute the same to the
Euclidean distance as the difference between two countries with GDP per
capita values of \$3000 and \$3001. Intuitively, we would not want such
a tiny difference in economic size to be treated as equally important as
belonging to different regions of the world. More generally, we want
differences across variables to be placed on a comparable scale, so that
a meaningful difference in one variable counts about the same as a
difference of similar importance in another.

Another concern with Euclidean distance is that it ignores correlations
among covariates. For example, countries with larger populations
(\texttt{pop}) usually have more military personnel (\texttt{milper}),
if only because a larger population provides a greater pool of potential
recruits. Therefore, differences in both covariates may largely reflect
the same underlying factor --- population size. Yet Euclidean distance
adds these differences separately, as if they were unrelated, which can
exaggerate the overall distance between two observations.

The Mahalanobis distance \citep{mahalanobis1936} addresses both of these
concerns. It handles the first by standardizing the covariates so they
are on a comparable scale. The Mahalanobis distance handles the second
by adjusting for correlations so that highly related variables are not
effectively counted twice. We can construct a distance matrix based on
Mahalanobis rather than Euclidean distances as follows.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Compute Mahalanobis distance matrix between treated (UN = 1) and control (UN = 0)}
\NormalTok{dist\_mat\_mah }\OtherTok{\textless{}{-}} \FunctionTok{match\_on}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ cov\_fmla,                 }
  \AttributeTok{data =}\NormalTok{ data,                  }
  \AttributeTok{standardization.scale =} \ConstantTok{NULL}\NormalTok{, }
  \AttributeTok{method =} \StringTok{"mahalanobis"}        \CommentTok{\# use Mahalanobis distance}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\doublespacing

\paragraph{Similarity on the Estimated Propensity
Score}\label{similarity-on-the-estimated-propensity-score}

So far, we have focused on ways to measure distance between treated and
control units across many covariates in order to identify which units
are most similar and group them together to achieve covariate balance.
However, when there are many covariates, it becomes difficult to find
treated and control units that are similar on all of them. This
challenge is often referred to as the ``curse of dimensionality.''

A common way to address this problem is to reduce the information from
many covariates into a lower-dimensional form. The \emph{estimated
propensity score} does this by collapsing information from all
covariates into a single number. This number represents a transformation
of a linear index of covariates that accounts for how strongly each
covariate predicts treatment.

Consider, for example, a logistic model for the estimated propensity
score of an individual unit \(i\). We write this model as
\begin{align} \label{eq: linear index}
\hat{\lambda}(\bm{x}_i) & \coloneqq \dfrac{1}{1 + \exp(-\bm{\hat{\beta}}^{\top}\bm{x}_i)},
\end{align} where \(\bm{x}_i\) is the covariate vector,
\(\bm{\hat{\beta}}\) is the vector of estimated coefficients, and the
superscript \(\top\) denotes transposition. The quantity
\(\bm{\hat{\beta}}^{\top}\bm{x}_i\) is the linear index, and the inverse
logistic function, \(1 / (1 + \exp(-x))\), maps any real-valued input,
\(x\), onto the interval \((0, \, 1)\). The linear covariate index for
unit \(i\), \(\bm{\hat{\beta}}^{\top}\bm{x}_i\), is simply the logit,
i.e., log odds, transformation of \(\hat{\lambda}(\bm{x}_i)\) in
\eqref{eq: linear index}.

This quantity, \(\hat{\lambda}(\bm{x}_i)\), is a simple transformation
of a linear index of covariates that best ``separates'' treated from
control units. The estimated coefficients (\(\bm{\hat{\beta}}\))
``separate'' treated from control units on the linear index because the
coefficients are chosen to maximize a likelihood that rewards large
differences between the groups. When treated and control units have
little or no covariate overlap, the linear indices can diverge
substantially, very positive for treated units and very negative for
controls. With greater overlap, the linear indices for treated and
control units are similar, clustering near zero.

This estimation process reflects how predictive each covariate is of
treatment. When treated and control observations lack overlap on a
covariate, that covariate is highly predictive of treatment and
therefore receives an estimated coefficient with a large magnitude. When
there is substantial overlap on a covariate, it is less predictive of
treatment, and the magnitude of its coefficient is small. Consequently,
when we assess similarity on the linear index of covariates in
\eqref{eq: linear index}, the estimated coefficients assign greater
importance to covariates that strongly predict treatment and less
importance to those that do not.

To see this logic in action, first estimate a logistic propensity score
model using all covariates except for \texttt{region}, which we exclude
because some regions almost perfectly predict treatment, leading to
near-complete separation \citep{albertanderson1984}.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Formula for UN \textasciitilde{} covariates (excluding "region")}
\NormalTok{psm\_cov\_fmla }\OtherTok{\textless{}{-}} \FunctionTok{reformulate}\NormalTok{(}\AttributeTok{termlabels =} \FunctionTok{setdiff}\NormalTok{(}\AttributeTok{x =}\NormalTok{ covs, }\AttributeTok{y =} \StringTok{"region"}\NormalTok{),}
                            \AttributeTok{response =} \StringTok{"UN"}\NormalTok{)}

\CommentTok{\# Fit logistic regression for propensity score model}
\NormalTok{psm }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ psm\_cov\_fmla,                 }\CommentTok{\# treatment \textasciitilde{} covariates}
  \AttributeTok{family  =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{), }\CommentTok{\# logistic regression (logit link)}
  \AttributeTok{data    =}\NormalTok{ data                      }\CommentTok{\# dataset used}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\doublespacing

We can extract units' linear covariate indices from this model like so.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract logit propensity scores (linear predictors from fitted model)}
\NormalTok{lin\_cov\_inds }\OtherTok{\textless{}{-}}\NormalTok{ psm}\SpecialCharTok{$}\NormalTok{linear.predictors  }\CommentTok{\# same as model.matrix(psm) \%*\% coef(psm)}
\end{Highlighting}
\end{Shaded}

\doublespacing

Below we can see that the linear covariate indices from the model
\texttt{psm} correspond exactly to the logits (i.e., the log-odds
transformations) of the model's predicted probabilities of treatment,
which range between \(0\) and \(1\).

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract estimated propensity scores (predicted probabilities of UN = 1)}
\NormalTok{p\_scores }\OtherTok{\textless{}{-}}\NormalTok{ psm}\SpecialCharTok{$}\NormalTok{fitted.values}

\CommentTok{\# Convert propensity scores to log{-}odds (logit scale)}
\FunctionTok{log}\NormalTok{(p\_scores}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p\_scores))}

\FunctionTok{all.equal}\NormalTok{(p\_scores, }\DecValTok{1}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{lin\_cov\_inds)))}
\end{Highlighting}
\end{Shaded}

\doublespacing

To see how two observations that differ on many covariates can still
have similar estimated propensity scores, consider post-conflict Namibia
(treated) and Burundi (control). The two are similar on some covariates,
such as logged military personnel (\texttt{milper}), but --- consistent
with the ``curse of dimensionality'' --- very different on others, such
as duration of the last war (\texttt{lwdurat}) and ethnic
fractionalization (\texttt{ethfrac}). Yet Namibia's and Burundi's linear
indices differ by only 0.5. This small difference occurs because the
covariates on which Namibia and Burundi differ greatly have small
coefficients (e.g., \texttt{lwdurat} \(\approx\) -0.01, \texttt{ethfrac}
\(\approx\) -0.29), while those on which the two observations are
similar, such as \texttt{milper}, have large coefficients (approximately
-0.92).

In some cases, differences on individual covariates may offset each
other; for example, when one covariate has a large negative coefficient
and another a large positive one. Conversely, even if a treated-control
pair is close in Euclidean distance --- say, closer than the 171.53
distance between Namibia and Burundi --- the two observations may still
differ more in their linear covariate indices if that pair differs on
covariates that are especially predictive of treatment.

To illustrate these broader patterns, the boxplot below compares the
empirical distributions of the linear covariate index for treated and
control groups.

\singlespacing

\begin{center}\includegraphics{Building_Design-Based_Matching_Pipeline_files/figure-latex/unnamed-chunk-11-1} \end{center}
\doublespacing

As the figure above shows, there is some, but not a lot of, overlap
between treated and control groups. In accordance with our earlier
discussion of how the linear covariate index ``separates'' treated from
control units, many control observations have very negative values (as
low as -7.52), far from the treated units' range of -2.24 to 2.21.
Nevertheless, a sufficient number of treated and control observations
have linear covariate indices that cluster around \(0\), indicating
covariate overlap for at least a subset of treated and control
observations.

\subsubsection{How Can I Apply Rules for Matches to Ensure
Comparability?}\label{how-can-i-apply-rules-for-matches-to-ensure-comparability}

The discussion above on measuring covariate distances between treated
and control observations helps identify which treated-control pairs are
similar. The goal of identifying these similar treated-control pairs is
to form matched sets that are closely aligned on their covariates,
thereby improving balance between treatment and control groups. In
practice, we do this by excluding potential matches that are ``too
dissimilar.'' There are two common ways to do this:

\begin{itemize}
\tightlist
\item
  \textbf{Exact matching}: Require that units be identical on some
  subset of important covariates, typically those that are categorical
  or coarse enough for units to take the same values.
\item
  \textbf{Calipers}: Impose a threshold for the maximum allowable
  distance so that no matched set may include a treated and a control
  unit that are farther apart than this caliper.
\end{itemize}

As a simple example to build intuition, we will impose the following
constraints:

\begin{itemize}
\tightlist
\item
  Observations can only be in the same matched stratum if they are in
  the same geographic region.
\item
  Treated and control observations more than two points apart on the
  Polity score cannot be in the same matched set.
\end{itemize}

Below we impose the first constraint, requiring an exact match on
region. Doing so produces separate distance matrices containing the
Euclidean distances on the covariate used to define the exact match
(\texttt{region}), where all entries are \(0\), indicating that treated
and control units belong to the same region.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create distance structure: 0 if units are in the same region, Inf otherwise}
\NormalTok{em\_region }\OtherTok{\textless{}{-}} \FunctionTok{exactMatch}\NormalTok{(}\AttributeTok{x =}\NormalTok{ UN }\SpecialCharTok{\textasciitilde{}}\NormalTok{ region,}
                        \AttributeTok{data =}\NormalTok{ data)}
\end{Highlighting}
\end{Shaded}

\doublespacing

Now we impose the second constraint: We construct a distance matrix
based on the Polity score, \texttt{bwplty2}, with a caliper of \(2\).
This matrix records the Euclidean difference in Polity scores when the
difference is \(2\) or less, and assigns a value of \(\infty\) (denoted
in \texttt{R} as \texttt{Inf}) when the difference exceeds \(2\). The
\(\infty\) entries are crucial because \texttt{optmatch} minimizes the
sum, across all matched sets, of the within-set sums of covariate
distances between each treated-control pair. Consequently, any
treated--control pair differing by more than 2 points on the Polity
score is assigned an overall distance of \(\infty\), which prevents them
from being matched.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Compute Euclidean distance matrix based on Polity score (bwplty2)}
\NormalTok{euc\_dist\_polity }\OtherTok{\textless{}{-}} \FunctionTok{match\_on}\NormalTok{(}\AttributeTok{x =}\NormalTok{ UN }\SpecialCharTok{\textasciitilde{}}\NormalTok{ bwplty2,}
                            \AttributeTok{data =}\NormalTok{ data,}
                            \AttributeTok{standardization.scale =} \ConstantTok{NULL}\NormalTok{,}
                            \AttributeTok{method =} \StringTok{"euclidean"}\NormalTok{)}

\CommentTok{\# Impose caliper of 2 on Polity score: any pair differing by more than 2 set to Inf}
\NormalTok{euc\_dist\_polity\_cal\_2 }\OtherTok{\textless{}{-}}\NormalTok{ euc\_dist\_polity }\SpecialCharTok{+} \FunctionTok{caliper}\NormalTok{(}\AttributeTok{x =}\NormalTok{ euc\_dist\_polity,}
                                                   \AttributeTok{width =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\doublespacing

Note that we used Euclidean distance here because, after exactly
matching on geographic region, matching proceeds on only \(1\) covariate
(Polity score), so we do not have to worry about covariates' relative
scales.

Finally, we combine the two constraints into distance matrices defined
within each region. We construct these region-specific matrices by
``adding'' the \texttt{em\_region} and
\texttt{euc\_dist\_polity\_cal\_2} objects, as shown below.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create overall distance matrix by element{-}wise addition of two distance matrices}
\NormalTok{overall\_dist\_mat }\OtherTok{\textless{}{-}}\NormalTok{ em\_region }\SpecialCharTok{+}\NormalTok{ euc\_dist\_polity\_cal\_2}
\end{Highlighting}
\end{Shaded}

\doublespacing

\subsubsection{How Can I Apply Rules for Matches to Improve Effective
Sample
Size?}\label{how-can-i-apply-rules-for-matches-to-improve-effective-sample-size}

Beyond comparability in covariates, we also care about the matched
study's effective size. The effective sample size --- i.e., how much
information the matched design provides --- depends not simply on the
total number of units included in our matches. Effective sample size
also depends on how those units are arranged across the matched sets.

The \texttt{optmatch} package will always produce matches with a
particular arrangement of units across sets. In particular, all sets
contain either \(1\) treated unit or \(1\) control unit --- an overall
structure that minimizes imbalance while excluding as few units as
possible \citep{rosenbaum1991, gurosenbaum1993, hansen2004}. In
practice, optimal full matching can yield lopsided sets, with either
\(1\) treated matched to many controls or \(1\) control matched to many
treated units, which has implications for the effective sample size.

The formal definition of effective sample size used by the
\texttt{optmatch} package is the sum, across matched sets, of the
harmonic mean of the numbers of treated and control units in each set:
\begin{align} \label{eq: harmonic mean}
\sum \limits_{s = 1}^S \left[\left(m_s^{-1} + (n_s - m_s)^{-1}\right)/2\right]^{-1}.
\end{align} In this definition, the index \(s\) runs over the
\(\left\{1, \dots , S\right\}\) matched sets, with \(m_{s}\) denoting
the number of treated units in set \(s\) and \(n_s - m_s\) the number of
control units. With \(n_s\) denoting the number of units in set \(s\),
the total number of individuals included in the matched study is
\(n = \sum_{s = 1}^S n_s\).

From the formula in \eqref{eq: harmonic mean}, we can see precisely how
the effective sample size depends on the arrangement of units across
sets. For example, in a study with \(4\) total units, the effective
sample size would be \(2\) if the units were arranged into \(2\) matched
pairs. By contrast, in a study of the same total size but arranged as a
single set with \(1\) treated unit and \(3\) controls, the effective
sample size would be \(1.5\). The effective sample size is larger in the
former arrangement because it provides \(2\) distinct
treated-versus-control comparisons, whereas the latter provides only
\(1\).

This definition of effective sample size connects directly to the
precision of estimators and the power of hypothesis tests. Assuming
constant, additive treatment effects within a set, the variance of the
Difference-in-Means --- the average outcome among treated units minus
the average outcome among control units --- in that set is minimized
when the harmonic mean is largest \citep{hansenbowers2008, hansen2011}.
The harmonic mean reaches its maximum when the numbers of treated and
control units are equal. Thus, all else equal, matched pairs and other
balanced sets provide more information about causal effects than
lopsided sets with unequal treated-to-control ratios.

One straightforward way to increase effective sample size is to relax
restrictions on which units can be matched. For instance, we might widen
the caliper on Polity score from \(2\) to \(3\) and then rebuild the
distance matrix.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Apply a caliper of width 3 to the polity Euclidean distance matrix}
\NormalTok{euc\_dist\_polity\_cal\_3 }\OtherTok{\textless{}{-}}\NormalTok{ euc\_dist\_polity }\SpecialCharTok{+} \FunctionTok{caliper}\NormalTok{(}\AttributeTok{x =}\NormalTok{ euc\_dist\_polity, }\AttributeTok{width =} \DecValTok{3}\NormalTok{)}

\CommentTok{\# Combine regional exact match distance with polity distance}
\NormalTok{em\_region }\SpecialCharTok{+}\NormalTok{ euc\_dist\_polity\_cal\_3}
\end{Highlighting}
\end{Shaded}

\doublespacing

Relaxing calipers allows more units to be included in the matched
design, since units without eligible matches would otherwise be
discarded. However, loosening calipers does not necessarily yield large
gains in effective sample size. The issue is that additional units
admitted by a looser caliper often cluster within a few sets. Because
those sets still contain only a single treated or control unit, the
extra observations contribute little to the effective sample size. In
other words, the additional units often provide more information where
it is least useful.

Instead of loosening calipers, we can shape the effective sample size by
using the \texttt{min.controls} and \texttt{max.controls} arguments in
\texttt{optmatch}'s \texttt{fullmatch()} function. These arguments set
lower and upper bounds, respectively, on the ratio of controls to
treated units allowed within each matched set. By default, the
\texttt{min.controls} argument is set to \texttt{0} and the
\texttt{max.controls} argument is set to \texttt{Inf}, imposing no
restrictions on the structure of the matched sets. By moving away from
these default values, researchers can control how balanced or lopsided
the sets may be.

To illustrate, suppose we want to restrict matches using the
\texttt{overall\_dist\_mat} introduced earlier. When we ultimately
construct our matches, full matching will divide the data into matched
sets containing one treated unit and any positive number of controls, or
one control unit and any positive number of treated units. However, we
can impose additional constraints on this full matching, such as
requiring at least \(1\) control for every \(2\) treated units
(\texttt{min.controls\ =\ 0.5}) and no more than \(2\) controls per
treated unit (\texttt{max.controls\ =\ 2}). Under these restrictions,
allowable matched sets could include \(2\) treated units with \(1\)
control, \(1\) treated unit with \(1\) control (a matched pair), or
\(1\) treated unit with \(2\) controls.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Full matching using overall distance matrix; allows 0.5 {-} 2 controls per treated}
\FunctionTok{fullmatch}\NormalTok{(}\AttributeTok{x =}\NormalTok{ overall\_dist\_mat, }\AttributeTok{min.controls =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{max.controls =} \DecValTok{2}\NormalTok{, }\AttributeTok{data =}\NormalTok{ data)}
\end{Highlighting}
\end{Shaded}

\doublespacing

Imposing ratio constraints can introduce trade-offs. In some cases,
balance may worsen if a control is forced to match with a less similar
treated unit --- though still within the specified calipers --- in order
to satisfy the minimum and maximum ratio rules. In other cases, such
constraints may improve effective sample size by redistributing how
units are grouped. However, if the restrictions are too stringent, they
can reduce effective sample size by forcing too many units to be
discarded.

In applied settings, final choices of calipers and ratio constraints
typically follow iterative checks of both covariate balance and
effective sample size. Practitioners often compare several
specifications to identify the most useful trade-off between these two
goals. \citet{hansensales2015} outline how this process can be carried
out in a structured way, drawing on the aforementioned
stepwise-intersection-union principle (SIUP) of hypothesis testing.

\subsubsection{How Do I Actually Form the
Matches?}\label{how-do-i-actually-form-the-matches}

The simple matching example above --- based on an exact match on
geographic region and a caliper on Euclidean distance for a single
covariate (Polity score) --- serves to illustrate the basic ideas. When
matching on many covariates, however, we will often prefer some
combination of Mahalanobis distance and propensity score matching,
sometimes adding calipers on specific covariates. In what follows, we
use \emph{rank-based} Mahalanobis distance, which has the advantage of
being less sensitive to outliers and differences in scales across
covariates \citep{rosenbaum2010}. We further constrain the matching by
imposing a caliper on the estimated propensity score, requiring treated
and control observations to come from the same geographic region, and
applying additional calipers directly to two covariates: ethnic
fractionalization (\texttt{ethfrac}) and logged GDP per capita
(\texttt{bwgdp}).

We impose a caliper equal to \(0.5\) standard deviations of the logit of
the estimated propensity score (the linear covariate index defined
above). This choice is larger than one rule of thumb emanating from
\citet{cochranrubin1973}, which recommends a caliper less than or equal
to \(0.20\) standard deviations. Given that the standard deviation of
the logit index is approximately 1.96, our choice of \(0.5\) permits
treated and control units to differ by up to roughly 0.98 units on the
logit scale, compared to about 0.39 under the \(0.20\) guideline.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add linear predictors from logistic regression (psm$linear.predictors) to dataset}
\NormalTok{data}\SpecialCharTok{$}\NormalTok{logit\_p\_score }\OtherTok{\textless{}{-}}\NormalTok{ lin\_cov\_inds}

\CommentTok{\# Population standard deviation of logit\_p\_score (divides by n, not n {-} 1)}
\NormalTok{pop\_sd\_logit }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{mean}\NormalTok{((data}\SpecialCharTok{$}\NormalTok{logit\_p\_score }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{logit\_p\_score))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}

\CommentTok{\# Distance matrix from propensity score (logit of estimated treatment probability)}
\NormalTok{ps\_mat }\OtherTok{\textless{}{-}} \FunctionTok{match\_on}\NormalTok{(}\AttributeTok{x =}\NormalTok{ UN }\SpecialCharTok{\textasciitilde{}}\NormalTok{ logit\_p\_score,}
                   \AttributeTok{caliper =} \FloatTok{0.5} \SpecialCharTok{*}\NormalTok{ pop\_sd\_logit, }\CommentTok{\# Apply 0.5 * SD caliper}
                   \AttributeTok{data =}\NormalTok{ data,}
                   \AttributeTok{method =} \StringTok{"euclidean"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\doublespacing

Below we construct the distance matrix for rank-based Mahalanobis
distance.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Rank{-}based Mahalanobis distance on covariates}
\CommentTok{\# covs was defined earlier as the set of covariate names; here we drop "region"}
\NormalTok{rank\_mah\_mat }\OtherTok{\textless{}{-}} \FunctionTok{match\_on}\NormalTok{(}
  \AttributeTok{x      =} \FunctionTok{reformulate}\NormalTok{(}\AttributeTok{termlabels =} \FunctionTok{setdiff}\NormalTok{(}\AttributeTok{x =}\NormalTok{ covs, }\AttributeTok{y =} \StringTok{"region"}\NormalTok{),}
                       \AttributeTok{response   =} \StringTok{"UN"}\NormalTok{),}
  \AttributeTok{data   =}\NormalTok{ data,                     }
  \AttributeTok{method =} \StringTok{"rank\_mahalanobis"} \CommentTok{\# compute rank{-}based Mahalanobis distance}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\doublespacing

Finally, we construct the Euclidean distance matrix for ethnic
fractionalization (\texttt{ethfrac}) and logged GDP per capita
(\texttt{bwgdp}) using calipers of \(0.35\) and \(2\), respectively. The
exact-match constraint on region has already been defined through the
object \texttt{em\_region} above.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Compute Euclidean distance matrix for ethnic fractionalization}
\NormalTok{eth\_mat }\OtherTok{\textless{}{-}} \FunctionTok{match\_on}\NormalTok{(}
\NormalTok{  UN }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ethfrac,}
  \AttributeTok{data =}\NormalTok{ data,}
  \AttributeTok{caliper =} \FloatTok{0.35}\NormalTok{, }\CommentTok{\# 0.35 caliper}
  \AttributeTok{method =} \StringTok{"euclidean"}
\NormalTok{)}

\CommentTok{\# Compute Euclidean distance matrix for logged GDP per capita}
\NormalTok{bwgdp\_mat }\OtherTok{\textless{}{-}} \FunctionTok{match\_on}\NormalTok{(}
\NormalTok{  UN }\SpecialCharTok{\textasciitilde{}}\NormalTok{ bwgdp,}
  \AttributeTok{data =}\NormalTok{ data,}
  \AttributeTok{caliper =} \DecValTok{2}\NormalTok{, }\CommentTok{\# 2.0 caliper}
  \AttributeTok{method =} \StringTok{"euclidean"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\doublespacing

We then combine the \texttt{ps\_mat}, \texttt{rank\_mah\_mat},
\texttt{eth\_mat}, \texttt{bwgdp\_mat}, and \texttt{em\_region} objects
to form the overall distance structure. We then pass the combined object
to \texttt{optmatch}'s \texttt{fullmatch()} function, imposing a
constraint that no more than \(4\) control units may be matched to any
treated unit. If instead we wanted to perform pair matching, the
\texttt{optmatch} package allows users to directly specify a
matched-pair structure via the \texttt{pairmatch()} function.
Equivalently, we could implement pair matching by setting
\texttt{min.controls\ =\ 1} and \texttt{max.controls\ =\ 1} in the
\texttt{fullmatch()} call. Below, we proceed with full matching under
the constraint that no more than \(4\) controls may be matched to any
treated unit.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Full matching on Mahalanobis + PS (with caliper) + region exact match}
\NormalTok{fm }\OtherTok{\textless{}{-}} \FunctionTok{fullmatch}\NormalTok{(}
  \AttributeTok{x            =}\NormalTok{ ps\_mat }\SpecialCharTok{+}\NormalTok{ rank\_mah\_mat }\SpecialCharTok{+}\NormalTok{ eth\_mat }\SpecialCharTok{+}\NormalTok{ bwgdp\_mat }\SpecialCharTok{+}\NormalTok{ em\_region,}
  \AttributeTok{data         =}\NormalTok{ data,                                        }
  \AttributeTok{max.controls =} \DecValTok{4} \CommentTok{\# up to 4 controls per treated; min.controls = 0 by default}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\doublespacing

To calculate the effective sample size of the matched observations, we
use the following function.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Effective sample size of matched sets}
\FunctionTok{effectiveSampleSize}\NormalTok{(fm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 15.43333
\end{verbatim}

\doublespacing

This reported effective sample size of approximately 15.43 is the sum
across sets of the within-set harmonic mean of the number of treated and
control subjects. This effective sample size reflects the matched
structure in which no set contains more than \(4\) controls for any
\(1\) treated observation.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Summarize matched sets (set sizes, structure) and report effective sample size}
\FunctionTok{summary}\NormalTok{(fm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Structure of matched sets:
1:0 2:1 1:1 1:2 1:3 1:4 0:1 
  6   1   4   3   3   1  44 
Effective Sample Size:  15.4 
(equivalent number of matched pairs).
\end{verbatim}

\doublespacing

The notation in the \texttt{summary()} output of \texttt{optmatch}
indicates the ratio of treated to control observations within each
matched set. For example, \texttt{1:0} indicates \(1\) treated unit and
no controls (effectively an unmatched treated unit). Similarly,
\texttt{1:2} indicates \(1\) treated unit and \(2\) controls, and
\texttt{0:1} indicates \(1\) control and no treated units (effectively
an unmatched control). Below each label, the output shows how many
matched sets have that particular structure.

If we examine the object (\texttt{fm}) returned by the matching call, we
see that \texttt{optmatch} labels each observation according to its
matched set, assigning \texttt{NA} to those not included. Because we
performed exact matching by region, \texttt{optmatch} labels each
matched set using the name of the exact-match stratum followed by a set
index within that stratum. For example, the label \texttt{lamerica.1}
denotes the first matched set within the Latin America stratum.

To see which units were matched together, we can add the \texttt{fm}
object to the dataframe and then tabulate. For example, to view the
\texttt{ssafrica.3} set, we run the following.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add matched set ID to data for each observation}
\NormalTok{data}\SpecialCharTok{$}\NormalTok{fm }\OtherTok{\textless{}{-}}\NormalTok{ fm}

\CommentTok{\# Look at one matched set ("ssafrica.3") for illustration}
\NormalTok{data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(fm }\SpecialCharTok{==} \StringTok{"ssafrica.3"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# keep only units in set "ssafrica.3"}
  \FunctionTok{select}\NormalTok{(cname, UN, region, logit\_p\_score, ethfrac, bwgdp, bwplty2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 7
  cname      UN region   logit_p_score ethfrac bwgdp bwplty2
  <chr>   <dbl> <fct>            <dbl>   <dbl> <dbl>   <dbl>
1 Burundi     0 ssafrica         0.508  0.0355  5.35      -7
2 Rwanda      1 ssafrica         0.148  0.129   5.68      -7
3 Somalia     0 ssafrica         0.209  0.0767  6.61      -7
4 Lesotho     0 ssafrica         0.558  0.222   6.28       0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# display selected variables}
\end{Highlighting}
\end{Shaded}

\doublespacing

We can see that the exact match on geographic region holds: All \(4\)
countries are located in Sub-Saharan Africa. The logit of the estimated
propensity score is similar across units, though not identical. The
treated country, Rwanda, is matched to three controls --- Burundi,
Somalia, and Lesotho --- and in each case the distance falls within
\(0.5\) standard deviations of the estimated logit propensity scores
(approximately 0.98), the caliper we specified. Likewise, Rwanda's
distances to each of the \(3\) control units fall within the calipers of
\(0.35\) for ethnic fractionalization (\texttt{ethfrac}) and \(2\) for
logged GDP per capita (\texttt{bwgdp}). By contrast, distances between
control countries may exceed these thresholds, since \texttt{optmatch}
enforces calipers only between treated and control units, not among
controls.

Some covariates used in the propensity score model and the rank-based
Mahalanobis distance --- such as Polity score (\texttt{bwplty2}) ---
still show modest imbalance within this matched set. This imbalance is
unsurprising. We applied a caliper on the logit of the estimated
propensity score and matched on the rank-based Mahalanobis distance
including \texttt{bwplty2}, but we did not apply a caliper directly to
\texttt{bwplty2}, though such a caliper could easily be added if
desired.

\subsubsection{How Do I Decide Whether to Move Ahead with My Matched
Design?}\label{how-do-i-decide-whether-to-move-ahead-with-my-matched-design}

Once we have constructed our matched sets, we want to evaluate the
overall quality of the matched design. Covariate balance is an important
aspect of this evaluation. To assess covariate balance, we compare the
balance in our matched observational study with the balance we would
expect to see in a completely randomized experiment within blocks
\citep{hansenbowers2008}.

Below we calculate the adjusted means (averaged across matched sets,
weighted by each set's contribution to the effective sample size) for
treatment and control groups, along with standardized differences (the
adjusted mean difference divided by the pooled standard deviation
calculated from the treatment and control groups combined).

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Install "RItools" package (only run if you don\textquotesingle{}t already have it installed)}
\CommentTok{\# install.packages("RItools")}

\CommentTok{\# Load RItools package for balance diagnostics (xBalance)}
\FunctionTok{library}\NormalTok{(RItools)}

\CommentTok{\# Compute covariate balance statistics with xBalance}
\NormalTok{cov\_bal }\OtherTok{\textless{}{-}} \FunctionTok{xBalance}\NormalTok{(}
  \AttributeTok{fmla =}\NormalTok{ cov\_fmla,}
  \AttributeTok{strata =} \FunctionTok{list}\NormalTok{(}
    \AttributeTok{unstrat =} \ConstantTok{NULL}\NormalTok{,   }\CommentTok{\# overall balance without stratification}
    \AttributeTok{fm      =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ fm    }\CommentTok{\# balance within matched sets}
\NormalTok{  ),}
  \AttributeTok{data =}\NormalTok{ data,  }\CommentTok{\# dataset containing treatment and covariates}
  \AttributeTok{report =} \FunctionTok{c}\NormalTok{(}\StringTok{"adj.means"}\NormalTok{, }\StringTok{"std.diffs"}\NormalTok{)}
  \CommentTok{\# Return adjusted means (weighted by effective sample size) }
  \CommentTok{\# and standardized differences (mean differences scaled by pooled SD)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\doublespacing

\singlespacing
\begin{table}[!h]
\centering
\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\begin{tabular}[t]{lcccccc}
\toprule
\multicolumn{1}{c}{\textbf{ }} & \multicolumn{3}{c}{\textbf{Before matching}} & \multicolumn{3}{c}{\textbf{After matching}} \\
\cmidrule(l{3pt}r{3pt}){2-4} \cmidrule(l{3pt}r{3pt}){5-7}
Covariate & Control mean & Treated mean & Std. diff & Control mean & Treated mean & Std. diff\\
\midrule
Log Cumulative Battle Deaths from Last War & 6.65 & 8.98 & 0.78* & 8.20 & 8.58 & 0.13\\
Duration of Last War & 50.28 & 80.53 & 0.38 & 63.67 & 72.36 & 0.11\\
ethfrac & 0.57 & 0.49 & -0.29 & 0.53 & 0.56 & 0.12\\
Log Population Size & 9.51 & 8.75 & -0.56* & 8.89 & 8.98 & 0.07\\
Log Mountainous & 2.22 & 2.80 & 0.42 & 2.53 & 2.66 & 0.09\\
Log Military Personnel & 3.87 & 3.25 & -0.38 & 3.53 & 3.53 & 0.00\\
Log GDP per Capita Before Last War & 6.56 & 6.59 & 0.03 & 6.64 & 6.45 & -0.18\\
Democracy (Polity Score) Before Last War & -0.84 & -2.58 & -0.31 & -2.18 & -2.74 & -0.10\\
Asia & 0.19 & 0.00 & -0.54* & 0.00 & 0.00 & 0.00\\
Eastern Europe & 0.15 & 0.37 & 0.57* & 0.40 & 0.40 & 0.00\\
Latin America & 0.12 & 0.21 & 0.27 & 0.09 & 0.09 & -0.00\\
North Africa \& Middle East & 0.12 & 0.11 & -0.04 & 0.06 & 0.06 & 0.00\\
Sub-Saharan Africa & 0.43 & 0.32 & -0.22 & 0.45 & 0.45 & 0.00\\
\bottomrule
\end{tabular}}
\end{table}
\doublespacing

\FloatBarrier

In the table above, the adjusted means offer a direct description of
balance. The standardized differences place all covariates on a common
scale, making imbalances comparable across variables. The stars indicate
cases where the adjusted mean difference would be unusually extreme
under complete random assignment within matched sets (based on a Normal
approximation to the distribution of the adjusted mean differences).
Because no adjustments are made for multiple comparisons, these stars
are conservative, meaning they are, if anything, more likely than the
nominal rate to detect a significant difference on a covariate.

One concern with balance tests is that high \(p\)-values may arise not
from improved covariate balance but from the reduction in effective
sample size that typically accompanies the matching process
\citep{austin2008, imaietal2008}. As \citet{hansen2008b} notes, however,
this possibility is less troubling than it first appears. The same
increase in standard errors that produces high \(p\)-values for
covariate balance tests will also carry over to subsequent causal
inferences, meaning that those high \(p\)-values remain informative:
They suggest that we are, if anything, less likely to overstate our
causal conclusions than if the \(p\)-values had been significant.

Regardless of whether the balance tests are statistically significant,
there are also established guidelines for what constitutes sufficient
balance. While precise thresholds depend on context and substantive
judgment about each covariate's importance, two commonly cited rules of
thumb appear in \citet{austin2009} and \citet{stuart2010}.
\citet{austin2009} suggests that standardized differences of 0.1 or
greater indicate inadequate balance on a covariate, whereas
\citet{stuart2010}, following \citet{rubin2001}, proposes a more lenient
threshold of 0.25. In our case, all covariates meet this latter
standard, and none show statistically significant differences,
indicating that observed imbalances are not unusual under a completely
randomized experiment within blocks (i.e., under as-if randomization).

In addition to assessing balance on each covariate individually,
\citet{hansenbowers2008} also propose an omnibus test that evaluates
balance across all covariates and their linear combinations
simultaneously. We conduct this test below.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{xBalance}\NormalTok{(}
  \AttributeTok{fmla   =}\NormalTok{ cov\_fmla,         }\CommentTok{\# formula: treatment \textasciitilde{} covariates}
  \AttributeTok{strata =} \FunctionTok{list}\NormalTok{(}\AttributeTok{fm =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ fm),  }\CommentTok{\# assess balance within matched sets (stratify by fm)}
  \AttributeTok{data   =}\NormalTok{ data,             }\CommentTok{\# dataset containing treatment, covariates, and fm}
  \AttributeTok{report =} \StringTok{"chisquare.test"}  \CommentTok{\# return chi{-}square test results for overall balance}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
---Overall Test---
   chisquare df p.value
fm      3.23  8   0.919
\end{verbatim}

\doublespacing

This \(\chi^2\) balance test yields an observed test statistic of 3.23
and a corresponding \(p\)-value of 0.92. Our high \(p\)-value indicates
that the observed level of covariate balance is consistent with what we
would expect in a completely randomized experiment within blocks.

Despite the high \(p\)-value, there is no guarantee that balance is
sufficient. Residual imbalance on observed covariates and hidden
imbalance on unobserved ones may undermine the as-if randomization
assumption. For now, we proceed under the as-if randomization
assumption, but we will later assess how sensitive our inferences are to
violations of it due to such imbalances.

\subsection{Part 2: Causal Inference from the Matched Design (under
As-If
Randomization)}\label{part-2-causal-inference-from-the-matched-design-under-as-if-randomization}

After forming matched sets that ideally satisfy the as-if randomization
assumption, researchers must decide which framework to use for
inference. In the sharp framework, inference pertains to unit-level
causal effects for every subject, thereby fully specifying the pattern
of counterfactual outcomes. The weak framework, by contrast, pertains to
a summary quantity of the unit-level causal effects, such as the ATE,
and thus admits multiple configurations of individual effects consistent
with it.

In both frameworks, the causal targets of inference are defined through
potential outcomes. Under the stable unit treatment value assumption
(SUTVA), each unit has two potential outcomes: a value the outcome would
take if that unit were assigned to treatment and a value the outcome
would take if that unit were assigned to control. Let \(y_{si}(1)\) and
\(y_{si}(0)\) denote these potential outcomes for unit \(i\) in set
\(s\), where the index \(i\) runs over the \(\{1, \ldots, n_s\}\) units
in set \(s\). The individual treatment effect is
\(\tau_{si} = y_{si}(1) - y_{si}(0)\). With \(n = \sum_{s = 1}^S n_s\)
total units, let
\(\bm{\tau} = (\tau_{1,1}, \tau_{1,2}, \ldots, \tau_{S,n_s})^{\top}\) be
the collection all \(n\) unit-level effects, and write the ATE as
\(\tau = (1/n) \sum_{s=1}^S \sum_{i=1}^{n_s} \tau_{si}\).

Neither \(\bm{\tau}\) nor \(\tau\) can be directly calculated. Even in a
randomized experiment, we cannot assign an individual to treatment,
measure the outcome, then rewind time to assign the same individual to
control and measure again. We therefore can observe only one potential
outcome per unit. Denote this observed outcome by \(y_{si}\), which is
the treated potential outcome when individual \(i\) in set \(s\) is in
the treatment condition or the control potential outcome when in the
control condition. Because we observe only one of each unit's two
potential outcomes, rather than the causal effect itself, we must rely
on statistical inference.

We consider inference under the sharp and weak frameworks, targeting
\(\bm{\tau}\) and \(\tau\), respectively. Ongoing work shows how both
types of effects can be inferred simultaneously under as-if
randomization
\citep{chungromano2013, ding2017, wuding2021, cohenfogarty2022}, though
they cannot generally be unified in sensitivity analyses
\citep{fogarty2023}. When researchers must choose between the two, the
decision depends on both statistical properties and substantive goals.

\begin{itemize}
\item
  Statistically, the sharp framework specifies all missing potential
  outcomes, allowing exact randomization inference under minimal
  assumptions. The weak framework leaves some outcomes unspecified and
  instead relies on variance estimation and a Normal approximation,
  which can perform poorly in small samples or when outcomes are skewed
  with extreme outliers. In such cases --- or whenever one wants exact
  \(p\)-values under minimal assumptions --- permutation inference under
  the sharp framework may be preferable.
\item
  Substantively, researchers usually test a constant effect in the sharp
  framework. Such a hypothesis may be unrealistic or of limited
  scientific interest \citep{gelman2003, gelman2011}. The weak
  framework, by contrast, accommodates heterogeneous effects across
  units, making the ATE a more relevant target in many settings.
  Nevertheless, testing a constant effect can provide a useful
  approximation to a more complex hypothesis with heterogeneous effects
  \citep[pp.~44--46]{rosenbaum2010}, and such tests remain valid for a
  range of bounded but heterogeneous effects \citep{caugheyetal2023}.
\end{itemize}

\paragraph{The Assignment Process as the Basis for
Inference}\label{the-assignment-process-as-the-basis-for-inference}

Regardless of the framework, inference is based on the treatment
assignment process. Let \(z_{si}\) denote an indicator for whether unit
\(i\) in matched set \(s\) is treated (\(z_{si} = 1\)) or not
(\(z_{si} = 0\)). We collect these indicators for all units in set \(s\)
into the vector
\(\bm{z}_s \coloneqq (z_{s1}, \ldots, z_{sn_s})^{\top}\), where the
superscript \(^{\top}\) denotes the transpose, turning the row vector
\((z_{s1}, \ldots, z_{sn_s})\) into a column vector. Stacking these
vectors across all sets gives the full assignment vector
\(\bm{z} = (z_{11}, \ldots, z_{Sn_S})^{\top}\). For inference, we
condition on the number of treated units within each set, even if the
actual assignment mechanism consisted of \(n_s\) independent
assignments. This conditioning represents a legitimate form of a
``conditional as-if analysis'' \citep{pashleyetal2021}.

We denote by \(\Omega_s\) the set of all possible treatment assignments
in set \(s\) that have a fixed number of treated units. Formally,
\(\Omega_s\) includes every possible way the \(n_s\) units in set \(s\)
could be assigned to treatment and control such that exactly \(m_s\)
units are treated. The number of possible assignments in \(\Omega_s\) is
denoted by \(\abs{\Omega_s}\), where the notation \(\abs{\cdot}\)
indicates the number of elements in a set. This quantity equals
\(\abs{\Omega_s} = \binom{n_s}{m_s} = \frac{n_s!}{m_s! (n_s - m_s)!}\),
where ``!'' denotes the factorial operator (e.g.,
\(4! = 4 \times 3 \times 2 \times 1\)).

In the \texttt{ssafrica.3} set, for example, there are \(4\)
observations --- \(1\) treated and \(3\) control --- so
\(\abs{\Omega_s}\) for the \texttt{ssafrica.3} set is
\(\binom{4}{1} = 4\). The corresponding set of possible assignments with
this treated count is shown in the table below.

\singlespacing
\vspace{1em}
\begin{center}
\begin{tabular}{l|cccc}
\hline
 & Assignment 1 & Assignment 2 & Assignment 3 & Assignment 4
 \\
\hline
Burundi & 0 & 0 & 0 & 1 \\
Rwanda & 0 & 0 & 1 & 0 \\
Somalia & 0 & 1 & 0 & 0 \\
Lesotho & 1 & 0 & 0 & 0 \\
\hline
\end{tabular}
\end{center}
\vspace{1em}
\doublespacing

The column labeled Assignment 3 shows the assignment that actually
occurred. The other possible assignments --- Assignment 1, Assignment 2,
and Assignment 4 --- represent cases in which Lesotho, Somalia, or
Burundi is treated instead of Rwanda. The set excludes any assignments
with more than \(1\) treated unit.

The set of possible treatment assignments across all matched sets, given
the number treated in each, is
\(\Omega \coloneqq \Omega_1 \times \ldots \times \Omega_S\), which is
all the ways one assignment can be chosen from each \(\Omega_s\) at the
same time. Although the assignment itself can vary, the underlying
causal quantities of interest --- \(\bm{\tau}\) and \(\tau\) --- remain
fixed across all possible assignments. What changes from one assignment
to another is which potential outcomes we actually observe. In the
observed Assignment 3, we see Rwanda's treated potential outcome and the
control potential outcomes of Burundi, Somalia, and Lesotho, but not
Rwanda's control potential outcome or the treated potential outcomes of
the others. Under a different assignment, a different set of treated and
control potential outcomes would have been observed. No matter which
assignment occurs, we observe only partial information about our causal
targets.

Inference from the partial information contained in the data to our
causal targets is predicated on a probability distribution defined over
the set of assignments, \(\Omega\). This distribution constitutes the
uncertainty underlying our inferences --- what \citet[p.~14]{fisher1935}
famously called the ``reasoned basis'' for inference --- but, unlike in
a randomized experiment, this distribution is unknown in an
observational study. Because this distribution is unknown, we must make
assumptions about it when drawing inferences from observational data.
Under the assumption of as-if randomization, every possible assignment
within each \(\Omega_s\) is equally likely, making each overall
assignment in \(\Omega\) equally likely as well. In this case, all
individuals within the same matched set have the same probability of
treatment. When as-if randomization does not hold, however, assignments
are no longer equally likely, and some individuals have a higher
probability of treatment than others.

\subsubsection{How Do I Draw Inferences under the Sharp
Framework?}\label{how-do-i-draw-inferences-under-the-sharp-framework}

To set the stage for inference under the sharp framework, consider a
thought experiment. Suppose we were to subtract the true individual
effects from the outcomes of the treated units. Doing so would yield,
for each unit, the outcome it would have had under control. In other
words, we may imagine reconstructing the dataset so that, under any
possible treatment assignment, the outcomes would appear exactly as they
would have if no one had been treated. In this reconstructed world,
there would be no effect since every unit's outcome would reflect what
it would have been without treatment.

Of course, we do not know the true collection of individual effects,
\(\bm{\tau}\), but we can test hypotheses about it, such as the
hypothesis of a homogeneous effect for all units, denoted by \(\tau_h\).
We do so by evaluating whether the data would look consistent with no
treatment effect when that hypothesized value is subtracted from the
treated outcomes. If, after this reconstruction, the data still show a
positive effect, then the hypothesized value is presumably too small; if
they show a negative effect, then the hypothesized value is presumably
too large.

More formally, when outcomes are reconstructed under a null hypothesis,
the observed test statistic will tend to fall in the upper tail of the
null distribution if the hypothesized effect is too small, and in the
lower tail if it is too large. To generate this null distribution, we
reconstruct the outcomes under the hypothesized effect. Under the null,
these reconstructed outcomes would remain fixed across assignments, so
we hold them constant and recalculate the test statistic for every
possible assignment.

A canonical choice of test statistic in this setting is a \emph{sum
statistic}, which first adds up the treated outcomes within each set,
and then adds those set-level sums across all sets. Many familiar test
statistics can be expressed in this form by applying to the outcomes
scale and shift transformations that do not depend on the treatment
assignments. For example, suppose we want the sum statistic from a test
of \(\tau_h = 0\) to reproduce the Difference-in-Means computed within
sets and then averaged across sets, with each set's contribution
weighted by its effective sample size. After reconstructing what the
treated outcomes would have been without treatment, as implied by the
null hypothesis \(\tau_h = 0\), we can rescale the outcomes as follows.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Keep only rows assigned to a matched set (drop NA in fm)}
\NormalTok{data\_matched }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(}\AttributeTok{.data =}\NormalTok{ data, }\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(fm))}

\CommentTok{\# Null hypothesis value}
\NormalTok{tau\_h }\OtherTok{\textless{}{-}} \DecValTok{0}

\CommentTok{\# Reconstruct outcomes under sharp null (tau\_h = 0)}
\NormalTok{data\_matched }\OtherTok{\textless{}{-}}\NormalTok{ data\_matched }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{ldur\_tilde =}\NormalTok{ ldur }\SpecialCharTok{{-}}\NormalTok{ tau\_h }\SpecialCharTok{*}\NormalTok{ UN)}

\CommentTok{\# Compute total harmonic weight H}
\NormalTok{H }\OtherTok{\textless{}{-}}\NormalTok{ data\_matched }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(fm) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarize}\NormalTok{(}
    \AttributeTok{m =} \FunctionTok{sum}\NormalTok{(UN }\SpecialCharTok{==} \DecValTok{1}\DataTypeTok{L}\NormalTok{),            }\CommentTok{\# treated count in set (1L = integer 1)}
    \AttributeTok{n\_minus\_m =} \FunctionTok{sum}\NormalTok{(UN }\SpecialCharTok{==} \DecValTok{0}\DataTypeTok{L}\NormalTok{),            }\CommentTok{\# control count in set (0L = integer 0)}
    \AttributeTok{h   =} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ m }\SpecialCharTok{+} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ n\_minus\_m),  }\CommentTok{\# set{-}specific harmonic mean weight}
    \AttributeTok{.groups =} \StringTok{"drop"}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{H =} \FunctionTok{sum}\NormalTok{(h), }\AttributeTok{.groups =} \StringTok{"drop"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pull}\NormalTok{(H)                    }\CommentTok{\# extract scalar}

\CommentTok{\# Build per{-}unit contributions to HM{-}weighted statistic}
\NormalTok{data\_matched }\OtherTok{\textless{}{-}}\NormalTok{ data\_matched }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(fm) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{m =} \FunctionTok{sum}\NormalTok{(UN }\SpecialCharTok{==} \DecValTok{1}\DataTypeTok{L}\NormalTok{),}
    \AttributeTok{n\_minus\_m =} \FunctionTok{sum}\NormalTok{(UN }\SpecialCharTok{==} \DecValTok{0}\DataTypeTok{L}\NormalTok{),}
    \AttributeTok{h   =} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ m }\SpecialCharTok{+} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ n\_minus\_m),}
    \AttributeTok{sum\_ldur\_tilde =} \FunctionTok{sum}\NormalTok{(ldur\_tilde), }\CommentTok{\# set{-}sum of adjusted outcomes}
    \CommentTok{\# Per{-}unit contribution:}
    \CommentTok{\# subtract scaled set{-}sum (so set diffs equal diff{-}in{-}means), weight by h\_s,}
    \CommentTok{\# then normalize by H}
    \AttributeTok{ldur\_tilde\_hm =}\NormalTok{ (ldur\_tilde }\SpecialCharTok{{-}}\NormalTok{ h }\SpecialCharTok{*}\NormalTok{ sum\_ldur\_tilde }\SpecialCharTok{/}\NormalTok{ (m }\SpecialCharTok{*}\NormalTok{ n\_minus\_m)) }\SpecialCharTok{/}\NormalTok{ H}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{m, }\SpecialCharTok{{-}}\NormalTok{n\_minus\_m, }\SpecialCharTok{{-}}\NormalTok{h, }\SpecialCharTok{{-}}\NormalTok{sum\_ldur\_tilde)          }\CommentTok{\# keep only contributions}

\CommentTok{\# Observed HM{-}weighted diff{-}in{-}means statistic}
\NormalTok{obs\_stat }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(data\_matched}\SpecialCharTok{$}\NormalTok{ldur\_tilde\_hm[data\_matched}\SpecialCharTok{$}\NormalTok{UN }\SpecialCharTok{==} \DecValTok{1}\DataTypeTok{L}\NormalTok{])}
\CommentTok{\# Equivalent to xBalance(fmla = UN \textasciitilde{} ldur, strata = list(fm = \textasciitilde{} fm),}
\CommentTok{\#                        data = data\_matched, report = "adj.mean.diffs")}
\end{Highlighting}
\end{Shaded}

\doublespacing

Then, to generate the distribution to which we refer our observed sum
statistic, we can hold the reconstructed and rescaled outcomes fixed and
then calculate the sum statistic over all possible assignments holding
the numbers of treated observations in each set fixed at their observed
values. In our application, with 12 sets ranging in size from 2 to 5,
the total number of assignments is 414720.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# For each matched set (fm), record:}
\CommentTok{\#   n   = total units in the set}
\CommentTok{\#   m = number treated (UN == 1)}
\NormalTok{block\_ns }\OtherTok{\textless{}{-}}\NormalTok{ data\_matched }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(fm) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{n   =} \FunctionTok{n}\NormalTok{(),}
    \AttributeTok{m =} \FunctionTok{sum}\NormalTok{(UN),}
    \AttributeTok{.groups =} \StringTok{"drop"}
\NormalTok{  )}

\CommentTok{\# Total possible treatment assignments = product of binomial coefficients}
\CommentTok{\# (choose n\_s units for treatment in each set and multiply across sets)}
\FunctionTok{prod}\NormalTok{(}\FunctionTok{choose}\NormalTok{(}\AttributeTok{n =}\NormalTok{ block\_ns}\SpecialCharTok{$}\NormalTok{n, }\AttributeTok{k =}\NormalTok{ block\_ns}\SpecialCharTok{$}\NormalTok{m))}
\end{Highlighting}
\end{Shaded}

\doublespacing

Because our matched study is relatively small, we can enumerate all
possible treatment assignments exactly.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Install "randomizr" (only run if not already installed)}
\CommentTok{\# install.packages("randomizr")}

\CommentTok{\# Load randomizr for generating random assignments}
\FunctionTok{library}\NormalTok{(randomizr)}

\NormalTok{exact\_assigns }\OtherTok{\textless{}{-}} \FunctionTok{obtain\_permutation\_matrix}\NormalTok{(}
  \AttributeTok{declaration =} \FunctionTok{declare\_ra}\NormalTok{(}
    \AttributeTok{N       =} \FunctionTok{nrow}\NormalTok{(data\_matched),  }\CommentTok{\# total number of units}
    \AttributeTok{blocks  =}\NormalTok{ data\_matched}\SpecialCharTok{$}\NormalTok{fm,     }\CommentTok{\# matched set membership}
    \AttributeTok{block\_m =}\NormalTok{ block\_ns}\SpecialCharTok{$}\NormalTok{m           }\CommentTok{\# number treated in each set}
\NormalTok{  ),}
  \AttributeTok{maximum\_permutations =} \FunctionTok{prod}\NormalTok{(}\FunctionTok{choose}\NormalTok{(}\AttributeTok{n =}\NormalTok{ block\_ns}\SpecialCharTok{$}\NormalTok{n, }\AttributeTok{k =}\NormalTok{ block\_ns}\SpecialCharTok{$}\NormalTok{m))  }
  \CommentTok{\# total number of feasible assignments across all matched sets}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\doublespacing

However, in most applications, exactly enumerating all possible
assignments is computationally infeasible. Instead, we typically draw a
random subset of assignments (e.g., \(10{,}000\)) to approximate the
exact randomization distribution, as illustrated below.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set RNG seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{11242017}\NormalTok{)}

\CommentTok{\# Generate permutation matrix of treatment assignments}
\CommentTok{\# Each column = one possible assignment consistent with block structure}
\NormalTok{sim\_assigns }\OtherTok{\textless{}{-}} \FunctionTok{obtain\_permutation\_matrix}\NormalTok{(}
  \AttributeTok{declaration =} \FunctionTok{declare\_ra}\NormalTok{(}
    \AttributeTok{N       =} \FunctionTok{nrow}\NormalTok{(data\_matched), }\CommentTok{\# total number of units}
    \AttributeTok{blocks  =}\NormalTok{ data\_matched}\SpecialCharTok{$}\NormalTok{fm,    }\CommentTok{\# matched set membership}
    \AttributeTok{block\_m =}\NormalTok{ block\_ns}\SpecialCharTok{$}\NormalTok{m        }\CommentTok{\# number treated in each set}
\NormalTok{  ),}
  \AttributeTok{maximum\_permutations =} \DecValTok{10}\SpecialCharTok{\^{}}\DecValTok{4}     \CommentTok{\# cap at 10,000 random draws}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\doublespacing

Now, to generate the null distribution of the sum statistic, we apply
the statistic to each of these \(10{,}000\) assignments while holding
the reconstructed and rescaled outcomes fixed under the null.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Randomization distribution under sharp null of no effect:}
\CommentTok{\# apply sum statistic to each assignment column in \textquotesingle{}assigns\textquotesingle{}}
\CommentTok{\# Outcome has been transformed so that }
\CommentTok{\# sum statistic = harmonic{-}mean weighted diff in means}
\NormalTok{sim\_sharp\_null\_dist }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(}
  \AttributeTok{X =}\NormalTok{ sim\_assigns,             }\CommentTok{\# matrix of treatment assignments}
  \AttributeTok{MARGIN =} \DecValTok{2}\NormalTok{,              }\CommentTok{\# iterate over columns (assignments)}
  \AttributeTok{FUN =} \ControlFlowTok{function}\NormalTok{(x) \{}
    \FunctionTok{sum}\NormalTok{(data\_matched}\SpecialCharTok{$}\NormalTok{ldur\_tilde\_hm[x }\SpecialCharTok{==} \DecValTok{1}\NormalTok{]) }\CommentTok{\# sum transformed outcomes among treated}
\NormalTok{  \}}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\doublespacing

From this null distribution, we can now compute a one-sided, upper
\(p\)-value as follows.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# One{-}sided, upper p{-}value: proportion of simulated randomization statistics \textgreater{}= observed}
\FunctionTok{round}\NormalTok{(}\AttributeTok{x =} \FunctionTok{mean}\NormalTok{(sim\_sharp\_null\_dist }\SpecialCharTok{\textgreater{}=}\NormalTok{ obs\_stat), }\AttributeTok{digits =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0279
\end{verbatim}

\doublespacing

The upper \(p\)-value of a test of the sharp null of no effect against
the alternative of a larger effect is 0.0279.

In this particular case --- unlike in most applications --- the matched
study is small enough to enumerate all possible assignments exactly,
allowing us to compute the exact \(p\)-value and assess the accuracy of
the simulation-based approximation.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Randomization distribution under sharp null of no effect:}
\CommentTok{\# apply sum statistic to each assignment column in \textquotesingle{}assigns\textquotesingle{}}
\CommentTok{\# Outcome has been transformed so that }
\CommentTok{\# sum statistic = harmonic{-}mean weighted diff in means}
\NormalTok{exact\_sharp\_null\_dist }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(}
  \AttributeTok{X =}\NormalTok{ exact\_assigns,             }\CommentTok{\# matrix of treatment assignments}
  \AttributeTok{MARGIN =} \DecValTok{2}\NormalTok{,              }\CommentTok{\# iterate over columns (assignments)}
  \AttributeTok{FUN =} \ControlFlowTok{function}\NormalTok{(x) \{}
    \FunctionTok{sum}\NormalTok{(data\_matched}\SpecialCharTok{$}\NormalTok{ldur\_tilde\_hm[x }\SpecialCharTok{==} \DecValTok{1}\NormalTok{]) }\CommentTok{\# sum transformed outcomes among treated}
\NormalTok{  \}}
\NormalTok{)}

\CommentTok{\# Exact one{-}sided, upper p{-}value: proportion of randomization statistics \textgreater{}= observed}
\FunctionTok{round}\NormalTok{(}\AttributeTok{x =} \FunctionTok{mean}\NormalTok{(exact\_sharp\_null\_dist }\SpecialCharTok{\textgreater{}=}\NormalTok{ obs\_stat), }\AttributeTok{digits =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0277
\end{verbatim}

\doublespacing

The exact \(p\)-value of 0.0277 is nearly identical to the
simulation-based \(p\)-value of 0.0279. At the conventional significance
level of \(\alpha = 0.05\), we would reject the null hypothesis in favor
of the alternative, regardless of whether we use the exact or
simulation-based \(p\)-value.

As an alternative to randomly sampling assignments and computing the
test statistic for each one, we can use a much faster Normal
approximation to the null distribution when the matched design is
sufficiently large. The approximation relies on closed-form expressions
for the expected value and variance of a sum statistic derived in
\citet{rosenbaumkrieger1990}. Using these expressions, we standardize
the observed test statistic and then compare it to the standard Normal
distribution, which gives us a corresponding \(p\)-value.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Install "senstrat" package (only run if you don\textquotesingle{}t already have it installed)}
\CommentTok{\# install.packages("senstrat")}

\CommentTok{\# Load senstrat for computing stratum{-}level null expectations/variances }
\CommentTok{\# (Rosenbaum \& Krieger 1990) and later sensitivity analysis}
\FunctionTok{library}\NormalTok{(senstrat)}

\CommentTok{\# Compute per{-}block null expectations and variances}
\NormalTok{per\_block\_moms }\OtherTok{\textless{}{-}}\NormalTok{ data\_matched }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(fm) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarize}\NormalTok{(}
    \AttributeTok{expect   =} \FunctionTok{ev}\NormalTok{(}
      \AttributeTok{sc     =}\NormalTok{ ldur\_tilde\_hm, }\CommentTok{\# transformed outcomes for the stratum}
      \AttributeTok{z      =}\NormalTok{ UN,            }\CommentTok{\# treatment indicator}
      \AttributeTok{m      =} \DecValTok{1}\NormalTok{,             }\CommentTok{\# number of "1"s in vector of hidden confounder}
                              \CommentTok{\# irrelevant here since Gamma = 1 (no hidden bias)}
      \AttributeTok{g      =} \DecValTok{1}\NormalTok{,             }\CommentTok{\# sensitivity parameter Gamma}
      \AttributeTok{method =} \StringTok{"RK"}           \CommentTok{\# Rosenbaum–Krieger (1990) formulas}
\NormalTok{    )}\SpecialCharTok{$}\NormalTok{expect,}
    \AttributeTok{variance =} \FunctionTok{ev}\NormalTok{(}
      \AttributeTok{sc     =}\NormalTok{ ldur\_tilde\_hm,}
      \AttributeTok{z      =}\NormalTok{ UN,}
      \AttributeTok{m      =} \DecValTok{1}\NormalTok{,             }\CommentTok{\# same as above: value doesn\textquotesingle{}t matter when Gamma = 1}
      \AttributeTok{g      =} \DecValTok{1}\NormalTok{,}
      \AttributeTok{method =} \StringTok{"RK"}
\NormalTok{    )}\SpecialCharTok{$}\NormalTok{vari,}
    \AttributeTok{.groups  =} \StringTok{"drop"}
\NormalTok{  )}

\CommentTok{\# Sum across blocks to get overall null expectation and variance}
\NormalTok{null\_ev  }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(per\_block\_moms}\SpecialCharTok{$}\NormalTok{expect)}
\NormalTok{null\_var }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(per\_block\_moms}\SpecialCharTok{$}\NormalTok{variance)}

\CommentTok{\# Standardized test statistic and one{-}sided Normal p{-}value (upper tail)}
\NormalTok{norm\_upper\_p\_value }\OtherTok{\textless{}{-}} \FunctionTok{pnorm}\NormalTok{(}
  \AttributeTok{q =}\NormalTok{ (obs\_stat }\SpecialCharTok{{-}}\NormalTok{ null\_ev) }\SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{(null\_var), }\CommentTok{\# standardized statistic under null}
  \AttributeTok{lower.tail =} \ConstantTok{FALSE}                         \CommentTok{\# compute upper{-}tail probability}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\doublespacing

This Normal-approximation \(p\)-value of 0.0278 is close to the exact
and simulation-based \(p\)-values of 0.0277 and 0.0279, respectively.
All \(p\)-values lead to the same conclusion in which we reject the
sharp null of no effect in favor of a larger effect.

To form a confidence set, one could invert the hypothesis test by
evaluating a grid of null hypotheses and retaining those not rejected at
the chosen significance level. Likewise, for a point estimate, one could
follow \citet{hodgeslehmann1963} and \citet{rosenbaum1993} by
identifying the null value that makes the observed sum statistic equal
to its null expectation. With our test statistic, this would occur when
the null equals the harmonic-mean weighted Difference-in-Means computed
from the observed outcomes (roughly 0.746), which yields a null
expectation of \(0\).

\subsubsection{How Do I Draw Inferences under the Weak
Framework?}\label{how-do-i-draw-inferences-under-the-weak-framework}

When the target is the ATE, it can be written as
\(\tau = \sum_{s = 1}^S (n_s / n)\tau_s\), a weighted average of the
set-level ATEs with weights equal to each set's share of the total study
size. Thus, a straightforward way to estimate the ATE is to compute the
Difference-in-Means within each matched set, and then combine these
set-level estimates using the same weights. Formally, the overall
Difference-in-Means is
\(\hat{\tau} = \sum_{s=1}^S (n_s / n) \hat{\tau}_s\), where
\(\hat{\tau}_s\) is the Difference-in-Means in set \(s\).

Under as-if randomization, the Difference-in-Means is an unbiased
estimator of the ATE. Formally, the expected value of the estimator ---
i.e.,, the average of \(\hat{\tau}\) taken over all treatment
assignments consistent with the matched design and their associated
probabilities --- equals the true ATE. Nevertheless, although correct in
expectation, the Difference-in-Means can vary substantially across
assignments. In any given assignment, the estimate may lie far from the
target ATE.

Because the estimator can fluctuate across different treatment
assignments, it is important to quantify the typical squared distance
between an estimate and the true ATE. To this end, \citet{neyman1923}
introduced a particular variance estimator. Under as-if randomization,
this estimator is exactly unbiased when individual treatment effects are
homogeneous; otherwise, it is conservative, meaning its expected value
is greater than or equal to the Difference-in-Means' true variance. In
principle, we could apply this approach by estimating the variance of
the Difference-in-Means within each matched set and then taking a
weighted sum of these estimates \citep[see, e.g.,][]{miratrixetal2013}.

In our setting, however, this approach is infeasible because each
matched set contains only \(1\) treated or \(1\) control unit. The usual
Neyman formula relies on computing sample variances separately within
the treated and control groups of each set. Sample variance requires at
least two observations because \texttt{var()} divides by the number of
observations minus \(1\). As a result, we cannot compute the sample
variance when there is only a single treated or control unit.

How, then, can we estimate the variance of the Difference-in-Means in a
matched observational study? \citet{pashleymiratrix2021} and
\citet{fogarty2018} offer distinct solutions.
\citet{pashleymiratrix2021} propose two approaches, each valid under
different conditions on the matched structure, while \citet{fogarty2018}
develops a single method that applies more broadly to finely stratified
designs.

The first approach in \citet{pashleymiratrix2021}, \texttt{hybrid\_m},
requires at least two matched sets of each unique set size in the data.
The second approach, \texttt{hybrid\_p}, permits variation in set sizes
as long as no single set contains half or more of the total study size.
Our matched design meets the condition for the second approach, not the
first. We therefore estimate the variance using \texttt{hybrid\_p}
through the \texttt{blkvar} package, the companion software for
\citet{pashleymiratrix2021}.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Install blkvar (only run if not already installed)}
\CommentTok{\# install.packages("remotes")}
\CommentTok{\# remotes::install\_github("lmiratrix/blkvar")}

\CommentTok{\# Load blkvar for block randomization variance estimators}
\FunctionTok{library}\NormalTok{(blkvar)}

\CommentTok{\# Compute results with hybrid\_p method}
\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{block\_estimator}\NormalTok{(}
  \AttributeTok{Yobs   =}\NormalTok{ ldur,         }\CommentTok{\# observed outcomes}
  \AttributeTok{Z      =}\NormalTok{ UN,           }\CommentTok{\# treatment indicator}
  \AttributeTok{B      =}\NormalTok{ fm,           }\CommentTok{\# block (matched set) membership}
  \AttributeTok{data   =}\NormalTok{ data\_matched, }\CommentTok{\# dataset}
  \AttributeTok{method =} \StringTok{"hybrid\_p"}    \CommentTok{\# hybrid estimator (pooled + separate blocks)}
\NormalTok{)}

\CommentTok{\# Extract variance estimate}
\NormalTok{res}\SpecialCharTok{$}\NormalTok{var\_est}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.144727
\end{verbatim}

\doublespacing

We can also estimate the variance of the Difference-in-Means in a finely
stratified design using the approach of \citet{fogarty2018}. Below, we
define a custom \texttt{R} function that implements this estimator. The
function takes as inputs the set-specific estimates and the numbers of
units in each set, and it returns a single scalar representing the
variance estimate.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# Args:}
  \CommentTok{\#   strat\_ns   : vector of set sizes n\_s (length B, positive integers)}
  \CommentTok{\#   strat\_ests : vector of set{-}specific estimates \textbackslash{}hat\{\textbackslash{}tau\}\_s (length B)}
  \CommentTok{\# Returns:}
  \CommentTok{\#   Conservative variance estimator (scalar), per Fogarty (2018, 2023)}

\NormalTok{fine\_strat\_var\_est }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(strat\_ns,}
\NormalTok{                               strat\_ests)\{}

  \CommentTok{\# {-}{-}{-}{-} Basic checks (shape, type, positivity) {-}{-}{-}{-}}
  \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{length}\NormalTok{(strat\_ns) }\SpecialCharTok{!=} \FunctionTok{length}\NormalTok{(strat\_ests))}
    \FunctionTok{stop}\NormalTok{(}\StringTok{"strat\_ns and strat\_ests must have the same length."}\NormalTok{)}
  
  \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{anyNA}\NormalTok{(strat\_ns) }\SpecialCharTok{||} \FunctionTok{anyNA}\NormalTok{(strat\_ests))}
    \FunctionTok{stop}\NormalTok{(}\StringTok{"strat\_ns and strat\_ests must not contain NA."}\NormalTok{)}
  
  \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{any}\NormalTok{(strat\_ns }\SpecialCharTok{\textless{}=} \DecValTok{0}\NormalTok{))}
    \FunctionTok{stop}\NormalTok{(}\StringTok{"All stratum sizes in strat\_ns must be positive."}\NormalTok{)}
  
\NormalTok{    strat\_ns   }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(strat\_ns)    }\CommentTok{\# coerce stratum sizes to numeric}
\NormalTok{  strat\_ests }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(strat\_ests)  }\CommentTok{\# coerce stratum estimates to numeric}

\NormalTok{  N }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(strat\_ns)    }\CommentTok{\# total sample size}
\NormalTok{  B }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(strat\_ns) }\CommentTok{\# number of strata (matched sets)}

  \CommentTok{\# Fogarty (2018, p. 1040) "Let W be a B x B diagonal matrix whose}
  \CommentTok{\# ith diagonal element contains the stratum weights w\_i = K (n\_i/N)"}
\NormalTok{  W }\OtherTok{\textless{}{-}} \FunctionTok{diag}\NormalTok{(B }\SpecialCharTok{*}\NormalTok{ (strat\_ns }\SpecialCharTok{/}\NormalTok{ N))          }\CommentTok{\# here K = B}

  \CommentTok{\# Fogarty (2018, p. 1040) "Let Q be an arbitrary B x L matrix with L \textless{} B"}
  \CommentTok{\# Fogarty (2023, p. 2199) "Let Q = (Q\_1, ... , Q\_B)\^{}T with Q\_i = B(n\_i/N)"}
\NormalTok{  Q }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(B }\SpecialCharTok{*}\NormalTok{ (strat\_ns }\SpecialCharTok{/}\NormalTok{ N))     }\CommentTok{\# B x 1 matrix}

  \CommentTok{\# solve(t(Q) \%*\% Q) = 1/S\_w}
\NormalTok{  S\_w }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(Q}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\CommentTok{\# scalar; equals t(Q)Q since Q is Bx1}

  \CommentTok{\# Fogarty (2023, p. 2199) "let H\_Q = Q(Q\^{}TQ)\^{}\{{-}1\}Q\^{}T be the hat matrix}
  \CommentTok{\# corresponding to Q"}
\NormalTok{  H\_Q }\OtherTok{\textless{}{-}}\NormalTok{ Q }\SpecialCharTok{\%*\%}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ S\_w) }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(Q) }\CommentTok{\# B x B projection matrix}

  \CommentTok{\# Fogarty (2018, p. 1040) "Define y\_i = \textbackslash{}hat\{\textbackslash{}tau\}\_i / sqrt(1 {-} h\_\{Qii)) as the}
  \CommentTok{\# estimated treatment effect in stratum i divided by the square root of 1 minus}
  \CommentTok{\# the leverage of  stratum i"}
  \CommentTok{\# Guard against tiny numerical negatives inside sqrt (due to rounding)}
\NormalTok{  leverages }\OtherTok{\textless{}{-}} \FunctionTok{pmin}\NormalTok{(}\FunctionTok{pmax}\NormalTok{(}\FunctionTok{diag}\NormalTok{(H\_Q), }\DecValTok{0}\NormalTok{), }\DecValTok{1}\NormalTok{)             }\CommentTok{\# clamp to [0,1]}
\NormalTok{  denom     }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{pmax}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ leverages, .Machine}\SpecialCharTok{$}\NormalTok{double.eps))}
\NormalTok{  y\_Gamma   }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(strat\_ests }\SpecialCharTok{/}\NormalTok{ denom)           }\CommentTok{\# B x 1 vector}

\NormalTok{  iden\_minus\_H\_Q }\OtherTok{\textless{}{-}} \FunctionTok{diag}\NormalTok{(}\AttributeTok{nrow =} \FunctionTok{nrow}\NormalTok{(H\_Q)) }\SpecialCharTok{{-}}\NormalTok{ H\_Q       }\CommentTok{\# I\_B {-} H\_Q}

  \CommentTok{\# Variance estimate: (1/B\^{}2) * y\^{}T W (I {-} H\_Q) W y   [scalar]}
\NormalTok{  var\_hat }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{((}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ B}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{*}
\NormalTok{                          (}\FunctionTok{t}\NormalTok{(y\_Gamma) }\SpecialCharTok{\%*\%}\NormalTok{ W }\SpecialCharTok{\%*\%}\NormalTok{ iden\_minus\_H\_Q }\SpecialCharTok{\%*\%}\NormalTok{ W }\SpecialCharTok{\%*\%}\NormalTok{ y\_Gamma))}
  
  \FunctionTok{return}\NormalTok{(var\_hat)}
  
  \CommentTok{\# Alternatively, as in Fogarty (2023, p. 1040), we could omit W and directly}
  \CommentTok{\# define y\_Gamma as}
  \CommentTok{\# y\_Gamma = as.matrix(x = (B * (strat\_ns/N) * strat\_ests) / sqrt(1 {-} diag(H\_Q)))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\doublespacing

After defining this function, we compute the set-specific estimates and
the corresponding set sizes, and then pass these two vectors as inputs
to our custom \texttt{R} function.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Compute stratum sizes and stratum{-}specific differences in means}
\NormalTok{strat\_stats }\OtherTok{\textless{}{-}}\NormalTok{ data\_matched }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(fm) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarize}\NormalTok{(}
    \AttributeTok{n =} \FunctionTok{n}\NormalTok{(),                               }\CommentTok{\# stratum size}
    \AttributeTok{diff\_in\_means =} \FunctionTok{mean}\NormalTok{(ldur[UN }\SpecialCharTok{==} \DecValTok{1}\DataTypeTok{L}\NormalTok{]) }\SpecialCharTok{{-}} \CommentTok{\# treated mean}
      \FunctionTok{mean}\NormalTok{(ldur[UN }\SpecialCharTok{==} \DecValTok{0}\DataTypeTok{L}\NormalTok{]),                }\CommentTok{\# minus control mean}
    \AttributeTok{.groups =} \StringTok{"drop"}
\NormalTok{  )}

\CommentTok{\# Apply Fogarty (2018/2023) variance estimator}
\FunctionTok{fine\_strat\_var\_est}\NormalTok{(}
  \AttributeTok{strat\_ns   =}\NormalTok{ strat\_stats}\SpecialCharTok{$}\NormalTok{n,              }\CommentTok{\# vector of stratum sizes}
  \AttributeTok{strat\_ests =}\NormalTok{ strat\_stats}\SpecialCharTok{$}\NormalTok{diff\_in\_means   }\CommentTok{\# vector of stratum estimates}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.1436324
\end{verbatim}

\doublespacing

This variance estimate is nearly identical to the one we obtained using
the \texttt{hybrid\_p} approach of \citet{pashleymiratrix2021}.

Now that we have estimates of both the ATE and the variance of the ATE
estimator, we can form a standardized test statistic by subtracting the
expected Difference-in-Means implied by the null and dividing the result
by the estimated standard error (the square root of the estimated
variance). We then compare this statistic to a standard Normal
distribution to calculate \(p\)-values. Below, we calculate the upper
one-sided \(p\)-value for a test the null hypothesis that the ATE is
\(0\) against the alternative that it is positive.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnorm}\NormalTok{(}
  \AttributeTok{q =}\NormalTok{ (res}\SpecialCharTok{$}\NormalTok{ATE\_hat }\SpecialCharTok{{-}} \DecValTok{0}\NormalTok{) }\SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{(res}\SpecialCharTok{$}\NormalTok{var\_est), }\CommentTok{\# standardized statistic}
  \AttributeTok{lower.tail =} \ConstantTok{FALSE}                         \CommentTok{\# upper{-}tail p{-}value}
  \CommentTok{\# By default: mean = 0, sd = 1 {-}\textgreater{} standard Normal distribution}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.03195766
\end{verbatim}

\doublespacing

We reject the weak null hypothesis that the ATE equals \(0\) in favor of
the alternative that the ATE is greater than \(0\), using the
conventional significance level of \(\alpha = 0.05\).

We can also form 95\% confidence sets by inverting these hypothesis
tests, keeping all null values that are not rejected at the
\(\alpha = 0.05\) level. In the two-sided case, we want the overall
false rejection rate to be 5\%. Testing the same null twice --- once
against a larger ATE and once against a smaller ATE --- would double
that rate to 10\%. To maintain a 5\% total error rate, we instead set
each tail's rejection region to \(\alpha/2 = 0.025\).

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alpha }\OtherTok{\textless{}{-}} \FloatTok{0.05}

\CommentTok{\# alpha (set earlier as alpha \textless{}{-} 0.05) is the significance level Lower bound of}
\CommentTok{\# 95\% CI: estimate {-} z\_(1 – alpha/2) x SE estimate}
\NormalTok{tau\_h\_lb }\OtherTok{\textless{}{-}}\NormalTok{ res}\SpecialCharTok{$}\NormalTok{ATE\_hat }\SpecialCharTok{{-}} \FunctionTok{qnorm}\NormalTok{(}\AttributeTok{p =} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alpha}\SpecialCharTok{/}\DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(res}\SpecialCharTok{$}\NormalTok{var\_est)}

\CommentTok{\# Upper bound of 95\% CI: estimate + z\_(1 {-} alpha/2) × SE estimate}
\NormalTok{tau\_h\_ub }\OtherTok{\textless{}{-}}\NormalTok{ res}\SpecialCharTok{$}\NormalTok{ATE\_hat }\SpecialCharTok{+} \FunctionTok{qnorm}\NormalTok{(}\AttributeTok{p =} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alpha}\SpecialCharTok{/}\DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(res}\SpecialCharTok{$}\NormalTok{var\_est)}

\CommentTok{\# Display interval, rounded to 2 decimals}
\FunctionTok{round}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(tau\_h\_lb, tau\_h\_ub), }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -0.04  1.45
\end{verbatim}

\doublespacing

Under as-if randomization, we conclude with 95\% confidence that the ATE
is no smaller than -0.04 and and no greater than 1.45.

\subsection{Part 3: Sensitivity Analysis for Hidden Confounding
(Departures from As-If
Randomization)}\label{part-3-sensitivity-analysis-for-hidden-confounding-departures-from-as-if-randomization}

Up to this point, we have supposed a framework in which each
observation's chance of a UN intervention is like flipping a weighted
coin. Each coin flip is independent across observations, but the
probability of landing tails (i.e., receiving treatment) can differ from
one unit to another depending on its baseline characteristics. With
matching, we make our crucial assumption that all units within a set are
similar enough on these characteristics that their coins have the same
probability of landing tails.

The as-if randomization assumption may fail because of imbalances on
hidden covariates (or residual imbalances on observed covariates, though
the sensitivity analysis to follow subsumes both within the same
framework). To represent such imbalances, consider a single hidden
covariate, \(\bm{u} = (u_{11}, \ldots , u_{Sn_S})^{\top}\) with each
\(u_{si}\) constrained to lie in the interval from \(0\) to \(1\).
Although the restriction that each element of \(\bm{u}\) is between 0
and 1 may seem strong, \citet[p.~300, fn. 33]{rosenbaum2017} shows that
any departure from complete random assignment within blocks can be
represented by such a \(\bm{u}\) in the sense that as-if randomization
would hold if we had access to and exactly matched on this \(\bm{u}\).

\citet{rosenbaum1987} and \citet{rosenbaumkrieger1990} then propose a
model in which each unit's independent probability of assignment to
treatment is given by \begin{equation}
\label{eq: indiv sens prob}
\pi_{si} \coloneqq \frac{\exp\left[\kappa_s + \log(\Gamma) u_{si}\right]}{1 + \exp\left[\kappa_s + \log(\Gamma) u_{si}\right]}.
\end{equation} The parameter \(\kappa_s\) is a set-specific intercept
that captures the baseline propensity for treatment shared by all units
in matched set \(s\) before accounting for any differences in the hidden
covariate. In other words, \(\kappa_s\) reflects the central idea in
matching that, after forming matched sets homogeneous in observed
covariates, all individuals within a set share the same treatment
propensity based on those covariates. The parameter \(\Gamma \geq 1\),
by contrast, quantifies how strongly the hidden covariate \(u_{si}\) can
alter treatment odds. When \(\Gamma = 1\), all individuals in matched
set \(s\) have the same probability of treatment,
\(\exp[\kappa_s] / (1 + \exp[\kappa_s])\). However, when \(\Gamma > 1\),
two individuals in the same set who differ in the hidden covariate may
differ in their odds of treatment by as much as a factor of \(\Gamma\).

An important point to reiterate is that we condition on assignments that
belong to the set \(\Omega\), meaning that the number of treated units
is fixed within each matched set. It turns out that this conditioning
removes dependence on the set-specific baseline \(\kappa_s\) in the
probability distribution over assignments in \(\Omega\). Eliminating
this dependence is crucial because it allows us to characterize both
as-if randomization and departures from it using just a single
sensitivity parameter, \(\Gamma\).

To build intuition for the sensitivity parameter \(\Gamma\), note that
the model in \eqref{eq: indiv sens prob} implies the following
restriction: \begin{equation} \label{eq: odds ratio restrict}
\frac{1}{\Gamma} \leq \frac{\pi_{si}\left(1 - \pi_{sj}\right)}{\pi_{sj}\left(1 - \pi_{si}\right)} \leq \Gamma \text{ for all } i, \, j \, \text{ and } s.
\end{equation} This restriction states that, within any set, no two
units can differ in their odds of treatment by more than a factor of
\(\Gamma\). When \(\Gamma = 1\), all units share the same odds of
treatment, corresponding to as-if randomization. By contrast, larger
values of \(\Gamma\) represent increasingly severe departures from this
assumption.

\citet[1424--1425]{rosenbaum1995a} shows that the converse also holds:
The restriction in \eqref{eq: odds ratio restrict} implies a model of
the form in \eqref{eq: indiv sens prob}. For any collection of treatment
probabilities satisfying the bound in \eqref{eq: odds ratio restrict},
it is always possible to find values of \(u_{si} \in [0,1]\) and a
scalar \(\Gamma \geq 1\) such that the two formulations yield the same
probability distribution over assignments in \(\Omega\). In other words,
\eqref{eq: odds ratio restrict} describes a general restriction on
treatment probabilities that does not assume any particular functional
form. The logistic model in \eqref{eq: indiv sens prob}, by contrast,
provides one convenient parametric representation of that general
restriction.

Thus far, we have conducted inference assuming \(\Gamma = 1\), meaning
that all units within a matched set share the same treatment
probability. Under this as-if randomization assumption, tests of both
sharp and weak null hypotheses are valid; that is, the probability of
rejecting the null does not exceed \(\alpha\). When we allow departures
from as-if randomization, governed by the sensitivity parameter
\(\Gamma \geq 1\), we seek new \(p\)-values that remain valid in the
same sense: The probability of a false rejection should not exceed
\(\alpha\), regardless of the hidden covariate \(\mathbf{u}\) or the
potential outcomes consistent with the null. The way we ensure this
validity, however, differs between sharp and weak nulls, which we
consider in turn.

\subsubsection{How Do My Inferences under the Sharp Framework Change
under these
Departures?}\label{how-do-my-inferences-under-the-sharp-framework-change-under-these-departures}

For a sharp null, if the hidden \(\bm{u}\) were known, we could
calculate the exact distribution of assignments consistent with the
matched design for any given value of \(\Gamma \geq 1\). This
distribution would provide the correct reference for computing
\(p\)-values under the null. In particular, the upper one-sided
\(p\)-value could be obtained by summing the probabilities of all
assignments whose test statistic under the null is at least as large as
the observed statistic.

In practice, \(\bm{u}\) is unknown, so, to ensure validity of our tests,
we compute \(p\)-values under the worst-case choice of \(\bm{u}\) ---
the one that makes the \(p\)-value as large as possible. Rejection under
the worst-case choice of \(\bm{u}\) guarantees rejection under the
actual (but hidden) \(\bm{u}\). Because a test based on the true
\(\bm{u}\) already controls the false rejection probability at or below
\(\alpha\), and the worst-case test can only reject as often or less
often, the false rejection probability under the worst-case choice of
\(\bm{u}\) must also be at most \(\alpha\).

\paragraph{Finding the Worst-Case Scenario of Hidden Confounding to
Ensure Valid
Inference}\label{finding-the-worst-case-scenario-of-hidden-confounding-to-ensure-valid-inference}

Actually finding this worst-case \(\bm{u}\) is difficult. As a first
step, \citet{rosenbaumkrieger1990} show that in the unmatched (i.e.,
two-sample) case, the vector \(\bm{u}\) that maximizes the \(p\)-value
under any fixed \(\Gamma \geq 1\) must belong to a restricted set of
possibilities, denoted \(U\). Specifically, once the subject are ordered
from the largest to the smallest outcome, the possible worst-case
\(\bm{u}\) vectors all look the same: a sequence of of \(1\)s at the top
followed by \(0\)s below. We do not know \emph{how many} \(1\)s should
precede the \(0\)s, and this number determines which configuration of
\(\bm{u}\) in \(U\) yields the worst-case \(p\)-value. Fortunately, in
an unmatched study, it is straightforward to enumerate all \(n - 1\)
such candidate vectors, where \(n - 1\) is the number of elements in
\(U\), and then identify which one yields the largest \(p\)-value for a
fixed \(\Gamma \geq 1\).

Unfortunately, in matched designs, the overall worst-case vector
\(\bm{u}\) cannot be obtained by simply stitching together the
worst-case vectors from each matched set. That is, the global worst-case
is not just the collection of local worst-cases. Instead, there are
\(\prod_{s = 1}^S (n_s - 1)\) total candidate vectors for the worst-case
\(\bm{u}\) --- a quantity that quickly becomes infeasible to enumerate
directly. For example, with only \(20\) matched sets of \(4\) units
each, the number of candidate vectors already exceeds \(3.5\) billion.

\subparagraph{Separable Approximation}\label{separable-approximation}

To address this challenge, \citet{gastwirthetal2000} propose a practical
shortcut called the \emph{separable approximation}. The idea is to
choose \(\bm{u}_s\) in \(U_s\) within each matched set separately,
selecting the one that maximizes the expected value of the test
statistic under the null. If more than one candidate yields the same
expectation, the choice goes to the \(\bm{u}_s\) that produces the
larger variance of the test statistic under the null. The method is
called ``separable'' because it then stitches together the choices of
\(\bm{u}_s\) made separately within each matched set, rather than
searching over all possible combinations across sets. The resulting
\(\bm{u}\) may not give the exact worst-case \(p\)-value; yet in designs
with many small matched sets, the error is negligible.

\subparagraph{Taylor Series
Approximation}\label{taylor-series-approximation}

The separable approximation is a useful shortcut, but can break down in
designs with only a moderate number of small strata or when most units
are concentrated in only a few strata. In such cases, the \(\bm{u}\)
chosen by the separable approximation may yield \(p\)-values that are
smaller than the worst-case \(p\)-value. To address this limitation,
\citet{rosenbaum2018} provides a refinement that guarantees valid
hypothesis tests.

The basic logic of this approach is as follows: Define a function that
takes as input a possible pattern of hidden bias across the matched sets
and outputs a number on the real line. The function maps onto hypothesis
testing in that, if we would reject the null hypothesis, then the value
the function produces will be less than or equal to \(0\); if we would
not reject the null, the value will be greater than \(0\). The essential
property is that this function is concave, which means that if you draw
its tangent line at any point, the line always lies above the curve
itself.

The specific configuration of \(\bm{u}\) selected by the separable
approximation serves as the expansion point. At that point, the function
and its tangent line coincide. The tangent line, however, defines a new
linear function over all possible configurations of \(\bm{u}\). Because
linear functions are straightforward to maximize, we can easily identify
the configuration of \(\bm{u}\) that makes the tangent line largest. And
since the tangent line always lies above the original function, if the
tangent line's worst-case value is at or below \(0\), then the original
function must also be at or below \(0\) for every configuration of
\(\bm{u}\), including its own (unknown) worst-case.

Although valid, this approach is conservative because sometimes the
tangent line stays above \(0\), even though the original function would
be below \(0\) at its worst-case \(\bm{u}\). In those situations, the
test will say ``do not reject,'' even though rejection would have been
justified. The separable approximation, by contrast, can be liberal: It
may lead to rejection even when the function at its worst-case
\(\bm{u}\) is positive.

\paragraph{Conducting Sensitivity Analysis under the Worst-Case
Scenario}\label{conducting-sensitivity-analysis-under-the-worst-case-scenario}

Below we illustrate a sensitivity analysis across different values of
\(\Gamma\), reporting two sets of \(p\)-values: one based on the
worst-case \(\bm{u}\) from the separable approximation and another based
on the worst-case \(\bm{u}\) from the Taylor series approximation. The
simplest recommendation is to use the latter, as it is the more
conservative of the two.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Grid of Gamma values}
\NormalTok{Gamma\_vals }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\AttributeTok{from =} \DecValTok{1}\NormalTok{, }\AttributeTok{to =} \FloatTok{1.5}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.0001}\NormalTok{)}

\CommentTok{\# Collect results for each Gamma}
\NormalTok{sens\_results }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(}
  \AttributeTok{X =}\NormalTok{ Gamma\_vals,}
  \AttributeTok{FUN =} \ControlFlowTok{function}\NormalTok{(g)\{}
\NormalTok{    out }\OtherTok{\textless{}{-}} \FunctionTok{senstrat}\NormalTok{(}\AttributeTok{sc =}\NormalTok{ data\_matched}\SpecialCharTok{$}\NormalTok{ldur\_tilde\_hm, }\CommentTok{\# outcome}
                    \AttributeTok{z  =}\NormalTok{ data\_matched}\SpecialCharTok{$}\NormalTok{UN,            }\CommentTok{\# treatment}
                    \AttributeTok{st =}\NormalTok{ data\_matched}\SpecialCharTok{$}\NormalTok{fm,            }\CommentTok{\# blocks}
                    \AttributeTok{gamma =}\NormalTok{ g,                       }\CommentTok{\# sens parameter (Gamma)}
                    \AttributeTok{alternative =} \StringTok{"greater"}\NormalTok{,}
                    \AttributeTok{detail =} \ConstantTok{TRUE}\NormalTok{)}
    \FunctionTok{data.frame}\NormalTok{(}
      \AttributeTok{Gamma =}\NormalTok{ g,}
      \AttributeTok{p\_sep =} \FunctionTok{as.numeric}\NormalTok{(out}\SpecialCharTok{$}\NormalTok{Separable[}\StringTok{"P{-}value"}\NormalTok{]),        }\CommentTok{\# separable approx}
      \AttributeTok{p\_tay =} \FunctionTok{as.numeric}\NormalTok{(out}\SpecialCharTok{$}\NormalTok{LinearBoundResult[}\StringTok{"P{-}value"}\NormalTok{]) }\CommentTok{\# Taylor approx}
\NormalTok{    )}
\NormalTok{  \})}

\CommentTok{\# Bind all rows into one data frame}
\NormalTok{sens\_df }\OtherTok{\textless{}{-}} \FunctionTok{do.call}\NormalTok{(}\AttributeTok{what =}\NormalTok{ rbind, }\AttributeTok{args =}\NormalTok{ sens\_results)}

\CommentTok{\# Smallest Gamma where the separable p{-}value is \textgreater{}= alpha}
\NormalTok{sens\_value\_sep }\OtherTok{\textless{}{-}} \FunctionTok{min}\NormalTok{(sens\_df}\SpecialCharTok{$}\NormalTok{Gamma[sens\_df}\SpecialCharTok{$}\NormalTok{p\_sep }\SpecialCharTok{\textgreater{}=}\NormalTok{ alpha])}

\CommentTok{\# Smallest Gamma where the Taylor p{-}value is \textgreater{}= alpha}
\NormalTok{sens\_value\_tay }\OtherTok{\textless{}{-}} \FunctionTok{min}\NormalTok{(sens\_df}\SpecialCharTok{$}\NormalTok{Gamma[sens\_df}\SpecialCharTok{$}\NormalTok{p\_tay }\SpecialCharTok{\textgreater{}=}\NormalTok{ alpha])}
\end{Highlighting}
\end{Shaded}

\doublespacing

For values of \(\Gamma\) ranging from 1 to 1.5, results from the
separable and Taylor series approximations are shown as distinct colored
lines on the same plot.

\singlespacing

\begin{center}\includegraphics{Building_Design-Based_Matching_Pipeline_files/figure-latex/unnamed-chunk-48-1} \end{center}
\doublespacing

For this matched dataset, the separable and Taylor-series approximations
yield nearly identical \(p\)-values for all values of \(\Gamma\) between
\(1\) and \(1.5\), with the two curves effectively overlapping. We can
reject the sharp null of no effect under as-if randomization (i.e.,
\(\Gamma = 1\)), but this conclusion is sensitive to departures from
that assumption. Under either approach, at a \(\Gamma\) of roughly
1.2281, we are no longer able to reject the sharp null of no effect
against the alternative of a larger effect at an \(\alpha\)-level of
\(0.05\).

\subsubsection{How Do My Inferences under the Weak Framework Change
under these
Departures?}\label{how-do-my-inferences-under-the-weak-framework-change-under-these-departures}

Compared to tests of the sharp null hypothesis of no effect,
constructing valid tests of an analogous weak null hypothesis is more
challenging. A sharp null hypothesis specifies all missing potential
outcomes, so we do not have to consider \(p\)-values over many different
configurations of potential outcomes. By contrast, a weak null permits
many possible configurations of potential outcomes, so identifying the
worst-case \(p\)-value over both \(\bm{u}\) and the potential outcomes
consistent with that null is often computationally intractable.

\citet{fogarty2023} proposes an alternative approach that yields valid
tests for the ATE (\(\tau\)) --- at least in sufficiently large matched
designs --- without requiring an explicit search for the worst-case
\(p\)-value across all configurations of \(\bm{u}\) and potential
outcomes consistent with the null hypothesis about \(\tau\). To build
intuition for this approach, note that the following weighted version of
the Difference-in-Means is unbiased for the ATE, \(\tau\):
\begin{align} \label{eq: weighted diff-in-means}
\sum \limits_{s = 1}^S (n_s/n) \left(\frac{1}{\abs{\Omega_s}}\right)\left(\frac{\hat{\tau}_s}{p(\bm{z}_s)}\right),
\end{align} where \(p(\bm{z}_s)\) denotes the probability of assignment
\(\bm{z}_s\) conditional on \(\bm{z}_s \in \Omega_s\). In practice, both
\(\hat{\tau}_s\) and \(p(\bm{z}_s)\) are evaluated at the same realized
treatment assignment \(\bm{z}_s\) within matched set \(s\). In other
words, \(p(\bm{z}_s)\) denotes the probability of the assignment vector
under which the observed treated and control outcomes in set \(s\) ---
and hence the Difference-in-Means \(\hat{\tau}_s\) --- were realized.

When \(\Gamma = 1\), this probability \(p(\bm{z}_s)\) is
\(1/\abs{\Omega_s}\), and the weighted Difference-in-Means reduces to
the usual unweighted Difference-in-Means, \(\hat{\tau}\). When
\(\Gamma > 1\), however, \(p(\bm{z}_s)\) is no longer known exactly.
Instead, \(\Gamma\) constrains the set of possible assignment
probabilities, conditional on \(\bm{z}_s \in \Omega_s\), to lie within
bounds consistent with the specified level of departure from as-if
randomization: \begin{align} \label{eq: prob bounds}
\dfrac{1}{\Gamma (n_s - 1) + 1} \leq p(\bm{z}_s) \leq \dfrac{\Gamma}{(n_s - 1) + \Gamma},
\end{align} for all \(\bm{z}_s \in \Omega_s\) and for all matched sets
\(s\).

Since the weighted Difference-in-Means in
\eqref{eq: weighted diff-in-means} cannot be directly computed when
\(\Gamma > 1\), \citet{fogarty2023} instead uses the bounds in
\eqref{eq: prob bounds} to construct a worst-case version of this
weighted Difference-in-Means. For tests of a null hypothesis about the
overall ATE across all matched sets, the procedure replaces
\(p(\bm{z}_s)\) with the upper bound from \eqref{eq: prob bounds}
whenever \(\hat{\tau}_s\) is greater than or equal to the null value of
the overall ATE, and with the lower bound whenever \(\hat{\tau}_s\) is
less than that null value. When testing the null against a smaller
alternative, the procedure reverses these substitutions.

To see the value of this procedure, consider testing a null hypothesis
about the ATE against the alternative of a larger ATE.
\citet{fogarty2023} shows that, for any \(\Gamma \geq 1\), the expected
value of the worst-case weighted Difference-in-Means --- defined under
whatever the true distribution on \(\Omega\) consistent with that
\(\Gamma\) happens to be --- is always less than or equal to the null
when it is true. Conversely, when testing against the alternative of a
smaller ATE, this expectation is greater than or equal to the null when
it is true. Importantly, these properties hold for all possible values
of \(\bm{u}\) and all configurations of potential outcomes that are
consistent with the null hypothesis.

How does this property of the expected value ensure a valid test in
sufficiently large studies? Consider testing the null hypothesis against
the alternative of a larger ATE. If the expected value of the worst-case
weighted Difference-in-Means is less than or equal to the null when it
is true, then the worst-case weighted Difference-in-Means tends to fall
at or below the null rather than above it. In other words, the procedure
intentionally ``tilts'' the worst-case weighted Difference-in-Means in
the direction opposite the alternative, making the procedure
conservative.

Now note that an analogue of the variance estimator from
\citet{fogarty2018} discussed above satisfies an important property. For
any fixed \(\Gamma \geq 1\), this estimator consistently overestimates
(or bounds above) the true variance of the worst-case weighted
Difference-in-Means. When we standardize the worst-case weighted
Difference-in-Means --- by dividing its deviation from the null by the
square root of this variance estimate --- the resulting value tends to
be smaller than it would be under the true (but unknown) variance.

Together, these two properties work in the same direction:

\begin{itemize}
\tightlist
\item
  The expected value is at or below the null, keeping the center of the
  distribution at or shifted away from the upper tail, and
\item
  the variance estimate tends to be too large, which pulls the
  standardized value closer to zero.
\end{itemize}

As a result, the probability that the standardized statistic falls in
the upper tail of the standard Normal distribution --- that is, the
probability of rejecting the null when it is true --- remains at or
below the nominal significance level. Hence, the test is conservative
(valid) in large studies. Analogous reasoning applies when testing
against the alternative of a smaller effect: In that case, the
probability that the standardized value falls in the lower tail of the
standard Normal distribution also stays at or below the nominal
significance level.

To implement this approach from \citet{fogarty2023}, we first source an
\texttt{R} function that, for a fixed \(\Gamma \geq 1\), computes the
worst-case weighted Difference-in-Means for a single matched set.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Worst{-}case IPW adjustment for a single matched set}
\CommentTok{\# Args:}
\CommentTok{\#   z           : 0/1 vector for one matched set (treated = 1, control = 0).}
\CommentTok{\#                 Set must be "finely stratified": exactly 1 treated OR 1 control.}
\CommentTok{\#   y           : numeric outcomes for same units.}
\CommentTok{\#   Gamma       : Rosenbaum sensitivity parameter (\textgreater{}= 1).}
\CommentTok{\#   tau\_h       : value of weak null ATE (default 0).}
\CommentTok{\#   alternative : "greater" (larger effect) or "less" (smaller effect).}
\CommentTok{\#}
\CommentTok{\# Returns:}
\CommentTok{\#   Worst{-}case IPW{-}weighted, set{-}specific (diff{-}in{-}means {-} tau\_h),}
\CommentTok{\#   multiplied by 1 / |Omega| where |Omega| = choose(N, \#treated).}
\CommentTok{\#}
\CommentTok{\# Probability bounds}
\CommentTok{\#   N \textless{}{-} length(z)}
\CommentTok{\#   lower = 1 / (Gamma * (n {-} 1) + 1)}
\CommentTok{\#   upper = Gamma / ((n {-} 1) + Gamma)}
\CommentTok{\#}
\NormalTok{worst\_case\_IPW }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(z,}
\NormalTok{                           y,}
\NormalTok{                           Gamma,}
                           \AttributeTok{tau\_h =} \DecValTok{0}\NormalTok{,}
                           \AttributeTok{alternative =} \FunctionTok{c}\NormalTok{(}\StringTok{"greater"}\NormalTok{, }\StringTok{"less"}\NormalTok{)) \{}

  \CommentTok{\# {-}{-}{-}{-} Basic checks {-}{-}{-}{-}}
\NormalTok{  alternative }\OtherTok{\textless{}{-}} \FunctionTok{match.arg}\NormalTok{(alternative)}
  \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{is.numeric}\NormalTok{(y) }\SpecialCharTok{||} \FunctionTok{length}\NormalTok{(y) }\SpecialCharTok{!=} \FunctionTok{length}\NormalTok{(z))}
    \FunctionTok{stop}\NormalTok{(}\StringTok{"y must be numeric and same length as z."}\NormalTok{)}
  \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{anyNA}\NormalTok{(z) }\SpecialCharTok{||} \FunctionTok{anyNA}\NormalTok{(y)) }\FunctionTok{stop}\NormalTok{(}\StringTok{"z and y must not contain NAs."}\NormalTok{)}
  \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{all}\NormalTok{(z }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{))) }\FunctionTok{stop}\NormalTok{(}\StringTok{"z must be a 0/1 vector."}\NormalTok{)}
  \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{is.numeric}\NormalTok{(Gamma) }\SpecialCharTok{||} \FunctionTok{length}\NormalTok{(Gamma) }\SpecialCharTok{!=} \DecValTok{1} \SpecialCharTok{||}\NormalTok{ Gamma }\SpecialCharTok{\textless{}} \DecValTok{1}\NormalTok{)}
    \FunctionTok{stop}\NormalTok{(}\StringTok{"Gamma must be a numeric scalar \textgreater{}= 1."}\NormalTok{)}

\NormalTok{  n\_treated }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(z)}
\NormalTok{  n\_control }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ z)}
\NormalTok{  N }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(z)}

  \CommentTok{\# Enforce finely stratified: exactly 1 treated OR exactly 1 control}
  \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\NormalTok{(n\_treated }\SpecialCharTok{==} \DecValTok{1}\DataTypeTok{L} \SpecialCharTok{||}\NormalTok{ n\_control }\SpecialCharTok{==} \DecValTok{1}\DataTypeTok{L}\NormalTok{)) \{}
    \FunctionTok{stop}\NormalTok{(}\StringTok{"Each matched set must have exactly 1 treated OR exactly 1 control."}\NormalTok{)}
\NormalTok{  \}}

  \CommentTok{\# Number of assignments with the fixed number treated}
\NormalTok{  card\_Omega }\OtherTok{\textless{}{-}} \FunctionTok{choose}\NormalTok{(}\AttributeTok{n =}\NormalTok{ N, }\AttributeTok{k =}\NormalTok{ n\_treated)}

  \CommentTok{\# {-}{-}{-}{-} Usual set{-}specific difference in means, centered at tau\_h {-}{-}{-}{-}}
\NormalTok{  diff\_means }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(y[z }\SpecialCharTok{==} \DecValTok{1}\NormalTok{]) }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(y[z }\SpecialCharTok{==} \DecValTok{0}\NormalTok{])}
\NormalTok{  d }\OtherTok{\textless{}{-}}\NormalTok{ (diff\_means }\SpecialCharTok{{-}}\NormalTok{ tau\_h)}

  \CommentTok{\# {-}{-}{-}{-} Uniform bounds on Pr(Z = z) for finely stratified set {-}{-}{-}{-}}
\NormalTok{  prob\_lb }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ (Gamma }\SpecialCharTok{*}\NormalTok{ (N }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}
\NormalTok{  prob\_ub }\OtherTok{\textless{}{-}}\NormalTok{ Gamma }\SpecialCharTok{/}\NormalTok{ ((N }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ Gamma)}

  \CommentTok{\# {-}{-}{-}{-} Worst{-}case IPW weight depending on alternative and sign of d {-}{-}{-}{-}}
  \ControlFlowTok{if}\NormalTok{ (alternative }\SpecialCharTok{==} \StringTok{"greater"}\NormalTok{) \{}
    \CommentTok{\# Favor rejection if d \textgreater{} 0 {-}\textgreater{} use 1 / upper; otherwise 1 / lower}
\NormalTok{    w }\OtherTok{\textless{}{-}} \ControlFlowTok{if}\NormalTok{ (d }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{) }\DecValTok{1} \SpecialCharTok{/}\NormalTok{ prob\_ub }\ControlFlowTok{else} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ prob\_lb}
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{ }\CommentTok{\# alternative == "less"}
    \CommentTok{\# Favor rejection if d \textless{} 0 {-}\textgreater{} use 1 / upper; otherwise 1 / lower}
\NormalTok{    w }\OtherTok{\textless{}{-}} \ControlFlowTok{if}\NormalTok{ (d }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{) }\DecValTok{1} \SpecialCharTok{/}\NormalTok{ prob\_ub }\ControlFlowTok{else} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ prob\_lb}
\NormalTok{  \}}

  \CommentTok{\# {-}{-}{-}{-} Multiply by 1 / |Omega| {-}{-}{-}{-}}
\NormalTok{  (}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ card\_Omega) }\SpecialCharTok{*}\NormalTok{ d }\SpecialCharTok{*}\NormalTok{ w}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\doublespacing

We now use this function to calculate the ``worst-case'' weighted
version of the Difference-in-Means within each matched set. To form the
overall statistic, we take a weighted average of the set-specific
values, using weights proportional to the number of units in each set.
Finally, we standardize this overall statistic using the conservative
variance estimator from \citet{fogarty2018}, and then compare the
resulting standardized test statistic to the standard Normal
distribution to obtain \(p\)-values.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# For each Gamma, compute one{-}sided ("greater") weak{-}null p{-}value}
\NormalTok{weak\_results\_list }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(}
  \AttributeTok{X =}\NormalTok{ Gamma\_vals,}
  \AttributeTok{FUN =} \ControlFlowTok{function}\NormalTok{(G)\{}

    \CommentTok{\# Per{-}set worst{-}case IPW contribution at Gamma, centered at tau\_h}
\NormalTok{    strat\_stats }\OtherTok{\textless{}{-}}\NormalTok{ data\_matched }\SpecialCharTok{|\textgreater{}}
      \FunctionTok{group\_by}\NormalTok{(fm) }\SpecialCharTok{|\textgreater{}}
      \FunctionTok{summarize}\NormalTok{(}
        \AttributeTok{n      =} \FunctionTok{n}\NormalTok{(),                }\CommentTok{\# set size n\_s}
        \AttributeTok{prop\_n =}\NormalTok{ n }\SpecialCharTok{/} \FunctionTok{nrow}\NormalTok{(data\_matched),    }\CommentTok{\# weight n\_s / n}
        \AttributeTok{est    =} \FunctionTok{worst\_case\_IPW}\NormalTok{(            }\CommentTok{\# set{-}level contribution d\_s * w\_s}
                   \AttributeTok{z =}\NormalTok{ UN,}
                   \AttributeTok{y =}\NormalTok{ ldur,}
                   \AttributeTok{Gamma =}\NormalTok{ G,}
                   \AttributeTok{tau\_h =} \DecValTok{0}\NormalTok{,}
                   \AttributeTok{alternative =} \StringTok{"greater"}
\NormalTok{                 ),}
        \AttributeTok{.groups =} \StringTok{"drop"}
\NormalTok{      )}

    \CommentTok{\# Overall statistic: weighted average across sets (weights n\_s / n)}
\NormalTok{    num }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(strat\_stats}\SpecialCharTok{$}\NormalTok{prop\_n }\SpecialCharTok{*}\NormalTok{ strat\_stats}\SpecialCharTok{$}\NormalTok{est)}

    \CommentTok{\# Conservative variance for finely stratified designs (Fogarty 2018)}
\NormalTok{    var\_hat }\OtherTok{\textless{}{-}} \FunctionTok{fine\_strat\_var\_est}\NormalTok{(}
      \AttributeTok{strat\_ns   =}\NormalTok{ strat\_stats}\SpecialCharTok{$}\NormalTok{n,}
      \AttributeTok{strat\_ests =}\NormalTok{ strat\_stats}\SpecialCharTok{$}\NormalTok{est}
\NormalTok{    )}

    \CommentTok{\# standardized statistic and one{-}sided p{-}value for "greater" alternative}
\NormalTok{    denom }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(var\_hat)}
\NormalTok{    z     }\OtherTok{\textless{}{-}} \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{is.finite}\NormalTok{(denom) }\SpecialCharTok{\&\&}\NormalTok{ denom }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{) num }\SpecialCharTok{/}\NormalTok{ denom }\ControlFlowTok{else} \ConstantTok{NA\_real\_}
\NormalTok{    pval  }\OtherTok{\textless{}{-}} \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{is.na}\NormalTok{(z)) }\ConstantTok{NA\_real\_} \ControlFlowTok{else} \FunctionTok{pnorm}\NormalTok{(}\AttributeTok{q =}\NormalTok{ z, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}

    \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Gamma =}\NormalTok{ G, }\AttributeTok{IPW =}\NormalTok{ num, }\AttributeTok{est\_sd =}\NormalTok{ denom, }\AttributeTok{z =}\NormalTok{ z, }\AttributeTok{p\_value =}\NormalTok{ pval, }\AttributeTok{stringsAsFactors =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{  \}}
\NormalTok{)}

\CommentTok{\# Bind rows: final results data.frame for plotting/reporting}
\NormalTok{weak\_sens\_df }\OtherTok{\textless{}{-}} \FunctionTok{do.call}\NormalTok{(}\AttributeTok{what =}\NormalTok{ rbind, }\AttributeTok{args =}\NormalTok{ weak\_results\_list)}

\CommentTok{\# Sensitivity value: smallest Gamma with p{-}value \textgreater{}= alpha}
\NormalTok{weak\_sens\_value }\OtherTok{\textless{}{-}} \FunctionTok{min}\NormalTok{(weak\_sens\_df}\SpecialCharTok{$}\NormalTok{Gamma[weak\_sens\_df}\SpecialCharTok{$}\NormalTok{p\_value }\SpecialCharTok{\textgreater{}=}\NormalTok{ alpha])}
\end{Highlighting}
\end{Shaded}

\doublespacing

Below, we plot the upper one-sided \(p\)-values corresponding to
\(\Gamma\) values from \(1\) to \(2\).

\singlespacing

\begin{center}\includegraphics{Building_Design-Based_Matching_Pipeline_files/figure-latex/unnamed-chunk-51-1} \end{center}
\doublespacing

Under as-if randomization (\(\Gamma = 1\)), we find evidence of a
positive average effect. This conclusion about the weak null is slightly
more robust to departures from as-if randomization than our earlier
rejection of the sharp null of no effect at \(\Gamma = 1\). However, in
absolute terms, the evidence for a positive ATE remains sensitive. Our
qualitative conclusion about the null hypothesis that the ATE equals
zero changes once \(\Gamma\) reaches 1.2741.

It is important to emphasize that this sensitivity analysis guarantees
validity across all configurations of potential outcomes consistent with
the weak null. By contrast, \citet{fogarty2023} also describes an
alternative sensitivity analysis that restricts attention to subsets of
configurations that researchers may regard as more plausible. Such
alternatives can produce smaller \(p\)-values, but at the cost of
forfeiting validity for certain (possibly unrealistic) outcome
configurations.

\section{Conclusion}\label{conclusion}

We have now completed the full matching pipeline. We began with the
construction of matched sets, followed by inference under the as-if
randomization assumption that the matched design is equivalent to a
collection of completely randomized experiments within blocks. We
examined inference under the sharp and weak frameworks ---
corresponding, respectively, to inference about unit-level effects and
the average effect. Finally, we considered the sensitivity of these
inferences to possible violations of the as-if randomization assumption.
The embedded code --- accompanied by extensive comments and illustrated
through the running example from \citet{gilligansergenti2008} --- should
be easily adapted by practitioners to their own datasets.

As we noted at the outset, this document does not cover several
important extensions. For example, we do not discuss how to incorporate
prognostic covariates or how to construct matches using alternative
methods, such as optimization procedures that directly target objectives
like covariate balance and effective sample size. Despite the omission
of these extensions, the overall pipeline and the design-based
principles it embodies provide a useful framework for understanding why
these extensions matter and how they can be incorporated when building
on the methods presented here.

\newpage

\bibliography{Bibliography.bib}

\end{document}
